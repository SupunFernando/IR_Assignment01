{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Supun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk library\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reminds', 'me', 'of', 'Liberal', 'Immigration', 'Fraudster', 'Monsef', 'avoiding', 'deportation', 'from', 'Canada', '.', '#cdnpoli', '#LPC', '#CPCLDR', '�', '�', '_', 'https://t.co/ZOZOSe1CqQ', '#immigration', '#integration', '#canada', 'https://t.co/M5cKGyvV8F', 'We', 'want', 'controlled', 'immigration', 'that', 'contributes', 'positively', 'to', 'the', 'UK', 'economy', '.', 'Same', 'as', 'Australia', '&', 'Canada', '.', 'https://t.co/99mYliuOes', 'Is', 'the', 'new', 'Manitoba', 'immigration', 'fee', 'a', 'head', 'tax', '?', 'https://t.co/LsG7C3vLe9', 'Canada', 'immigration', 'profit', 'influence', 'modernistic', 'delhi', 'yet', 'abhinav', ':', 'XKofy', 'https://t.co/becgusY2i6', 'Canada', 'Immigration', 'Minister', 'to', '�', '�', '�', 'Substantially', 'Increase', 'Immigration', 'Numbers', 'https://t.co/nEFw30MRaa', 'https://t.co/cyI867PZRV', 'M', '�', '�', 'me', 'les', '#USA', '=p', 'ays', \"d'immigration\", 'par', 'excellence', 'CONTR', '�', '�', 'LE', 'RIGOUREUSEMENT', \"l'immigration\", 'et', 'acc', '�', '�', 's', '�', '�', 'la', '#GreenCARD', '!', '�', '�', '_', 'https://t.co/IHpVhW2BaG', '@Shawhelp', 'what', 'changes', 'should', 'be', 'made', 'to', \"Canada's\", 'immigration', 'laws', 'due', 'to', 'the', 'influx', 'of', 'immigration', 'and', 'violence', '?', 'L', '�', '�', 'immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', 'questions', 'https://t.co/f4utO5A7ZF', \"L'immigration\", 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', 'questions', '-', 'https://t.co/UiBsEZOqas', 'https://t.co/j77dEvjoiX', 'https://t.co/XXDeIG7Dbu', 'Will', 'Media', 'ask', 'the', 'Liberals', 'if', 'they', 'actually', 'have', 'a', 'solid', 'plan', 'for', 'Canada', '_', '�', '�', '_', '�', '_', '?', '?', 'From', 'my', 'view', '-', '-', 'immigration', 'out', 'of', 'C', '�', '�', '_', 'https://t.co/YAgwmZ8ECp', 'Dan', 'Murray', 'of', '�', '�', 'Immigration', 'Watch', 'Canada', 'is', 'xenophobic', 'racist', 'fear-mongering', 'liar', '#racism', '#canada', '#cdnpoli', '#hatecrime', '�', '�', '_', 'https://t.co/kwZ3csvYxM', 'Le', 'Canada', 'lance', 'une', 'vaste', 'campagne', \"d'immigration\", 'pour', 'faire', 'face', '�', '�', 'son', 'besoin', 'de', 'main', 'd', \"'\", '�', '�', 'uvre', 'https://t.co/kXdfMGTZzN', 'L', '�', '�', '#immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', '#Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', '�', '�', 'questions', 'https://t.co/s3hu1OKKIG', '@Canadidly', \"I've\", 'read', 'the', 'Immigration', 'laws', 'of', 'Canada', 'much', 'stricter', 'than', 'the', 'US', 'Canada', 'Immigration', 'Website', 'Traffic', 'Surges', 'And', 'Crashes', 'In', 'Wake', 'Of', 'Trump', '#fasttraffic', ',', '#sitetraffic', ',', '#website', ',', '#traffic', 'https://t.co/zRlJ26jnkC', 'Mr', 'Know-all', 'of', 'Canada', 'Immigration', 'https://t.co/wTQK4QDiKI', 'Move', 'to', 'Canada', '@LadyMadonna___', 'Oh', ',', 'immigration', 'rules', ',', 'you', \"can't\", '...', 'https://t.co/5LIEVHO7A4', '#OnThisDay', 'Annette', 'Toft', 'becomes', \"Canada's\", '2', 'millionth', 'immigrant', 'since', '1945', '.', 'Do', 'you', 'know', 'your', \"family's\", 'immigration', 'st', '�', '�', '_', 'https://t.co/UvRuw8eR1b', '.', '@TheEconomist', 'profiles', \"Canada's\", 'open', 'immigration', 'policies', '&', 'how', 'they', 'contribute', 'to', 'our', 'economic', 'success', ':', '�', '�', '_', 'https://t.co/4K84EE8Y63', 'Hundreds', 'may', 'lose', 'Canadian', 'citizenship', ',', 'resident', 'status', 'because', 'of', 'one', 'corrupt', 'immigration', 'consultant', 'https://t.co/x2IfO0EXI2', 'Immigration', 'for', 'canada', 'without', 'india', ':', 'an', 'compassionate', 'handle', ':', 'deyFy', '\"', '#Jamaican', '#immigrants', '#Canada', 'https://t.co/vcmfYGadR5', '#statistics', '#immigration', '\"', 'Mexican', 'visa', 'lift', 'expected', 'to', 'cost', 'Canada', '$', '262M', 'over', 'a', 'decade', 'https://t.co/9i72fRhtij', 'Are', 'people', 'still', 'moving', 'to', '#Canada', '?', '?', '?', 'Oh', \"that's\", 'right', ',', 'they', 'have', 'real', 'immigration', 'laws', 'and', \"it's\", '�', '�', '_', 'https://t.co/0C5OBfmxLG', 'Here', 'are', 'more', 'details', 'on', 'the', 'Richmond', ',', 'B', '.', 'C', '.', 'Immigration', 'Consultant', 'Sunny', 'Wang', 'who', 'was', 'sentenced', 'to', '7', 'years', 'in', '...', 'https://t.co/YXH5W53srO', 'I', 'added', 'a', 'video', 'to', 'a', '@YouTube', 'playlist', 'https://t.co/CnEyWN40x3', 'Funny', 'Talking', 'of', 'Haryanavi', 'Jat', 'with', 'Canada', 'Immigration', 'Girl', 'Agent', 'Mexicans', 'Can', 'Now', 'Travel', 'Visa-Free', 'To', 'Canada', 'https://t.co/Ec3XHORO2s', 'https://t.co/RQRr5nebcG', 'L', '�', '�', 'immigration', 'irr', '�', '�', 'guli', '�', '�', 're', 'au', 'Canada', 'd', '�', '�', 'cortiqu', '�', '�', 'e', 'en', '5', '�', '�', 'questions', 'https://t.co/DkpuKyWmaK', '@SweetnessShawnB', 'Hes', 'the', 'POS', 'that', 'ramped', 'up', 'immigration', 'for', 'Canada', ',', 'among', 'other', 'globalist', 'policies', '.', 'Canada', 'lifted', 'visa', 'requirements', 'to', 'Mexico', 'as', 'of', 'Dec', '1', ',', '2016', '.', 'Thoughts', '?', '#visa', '#immigration', '@HuffingtonPost', 'people', 'Keep', 'praising', 'Canada', 'and', 'Canada', 'has', 'way', 'stricter', 'immigration', 'laws', 'then', 'us', 'they', 'willl', 'boot', 'your', 'liberal', 'American', 'ass']\n"
     ]
    }
   ],
   "source": [
    "#read twitter.txt file\n",
    "tweeterFile = open(\"C:\\\\Users\\\\Supun\\\\Desktop\\\\Supun Backup\\\\Supun\\\\transfer\\\\documents\\\\myfiles\\\\MScDA\\\\DS\\\\IR\\\\Assignment_1\\\\twitter.txt\", \"r\", encoding='utf-8')\n",
    "twitterdata = tweeterFile.read()\n",
    "\n",
    "#tokenize\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "token_twitter = tweet_tokenizer.tokenize(twitterdata)\n",
    "\n",
    "print(token_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Student feedback data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Honestly', 'last', 'seven', 'lectures', 'are', 'good', '.', 'Lectures', 'are', 'understandable', '.', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self-study', 'also', '.', 'The', 'given', 'opportunity', 'to', 'ask', 'questions', 'from', 'the', 'lecturer', 'is', 'appreciative', '.', '``', 'Good', ':', ')', '<', 'br', '/', '>', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', '&', '#', '039', ';', 's', 'better', 'for', 'us', '.', '<', 'br', '/', '>', 'sometimes', 'teaching', 'speed', 'is', 'very', 'high', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Thanks', '!', ':', ')', '<', 'br', '/', '>', '``', 'The', 'lectures', 'are', 'good', '..', 'but', 'a', 'bit', 'speed.A', 'in', 'class', 'working', 'activity', 'is', 'a', 'must', 'one.So', 'please', 'take', 'another', 'hour', 'in', 'thursdays', 'madame.', \"''\", '<', 'br', '/', '>', 'We', 'can', 'hear', 'your', 'voice', 'clearly', 'and', 'can', 'understand', 'the', 'things', 'you', 'teach', '.', 'Presentation', 'slides', 'also', 'good', 'source', 'to', 'refer', '.', 'lf', 'you', 'can', 'do', 'more', 'example', 'questions', 'within', 'the', 'classroom', 'and', 'it', 'will', 'help', 'us', 'to', 'understand', 'the', 'principles', 'well', '.', '<', 'br', '/', '>', \"''\", 'Lectures', 'was', 'well', 'structured', 'and', 'well', 'organized', '.', 'It', 'was', 'easy', 'to', 'understand', '.', 'Lecture', 'slides', 'and', 'labs', 'were', 'also', 'well', 'organized', '.', 'Lectures', 'were', 'good', '.', 'understandable', '.', 'The', 'lecture', 'slides', 'were', 'well', 'organized', 'and', 'the', 'examples', 'done', 'in', 'the', 'class', 'helped', 'a', 'lot', 'to', 'learn', 'this', 'new', 'language', 'and', 'also', 'the', 'principles', 'of', 'OOP', '.', 'Motivated', 'to', 'well', '.', 'Would', 'have', 'been', 'better', 'if', 'we', 'discussed', 'more', 'about', 'the', 'solutions', 'of', 'coding', 'exercisers', '.', 'I', 'think', 'i', 'learned', 'a', 'lot', 'from', 'the', 'codes', 'you', 'write', 'in', 'the', 'board', '.', 'When', 'i', 'compare', 'my', 'codes', 'with', 'yours', 'i', 'can', 'learn', 'about', 'my', 'mistakes', 'and', 'good', 'coding', 'practices', 'that', 'i', 'should', 'follow', '.', 'There', 'fore', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'we', 'can', 'discuss', 'more', 'examples', 'in', 'the', 'class', '.', 'madam', 'explained', 'the', 'oop', 'concepts', 'clearly', 'with', 'examples.lectures', 'were', 'interesting.we', 'want', 'more', 'scenario', 'examples', 'and', 'answers', 'with', 'explanations', 'in', 'future', '.', 'I', 'satisfy', 'about', 'first', '7', 'lectures', '.', 'That', 'way', 'of', 'teaching', 'is', 'really', 'good', 'for', 'coming', 'lectures', 'too', '.', 'lectuers', 'are', 'very', 'good', '.', 'take', 'good', 'effort', 'to', 'make', 'undersatand', 'every', 'student', 'in', 'the', 'room', '.', 'very', 'helpfull', '.', 'I', 'was', 'able', 'to', 'obtain', 'a', 'clear', 'picture', 'about', 'OOP', 'and', 'its', 'concepts', '.', '``', 'lecture', 'slides', ',', 'explanations', 'were', 'very', 'clear', '.', '<', 'br', '/', '>', 'it', '&', '#', '039', ';', 's', 'very', 'good', 'to', 'letting', 'ask', 'questions', 'and', 'explain', 'again', 'with', 'suitable', 'examples', '.', '<', 'br', '/', '>', 'sometimes', ',', 'some', 'codes', 'on', 'white', 'board', 'were', 'unclear', 'at', 'the', 'back', '.', '<', 'br', '/', '>', 'overall', 'very', 'good', '!', '!', '!', '<', 'br', '/', '>', \"''\", 'The', 'lectures', 'were', 'good', 'and', 'clear', '.', 'And', 'they', 'weren', '&', '#', '039', ';', 't', 'too', 'fast', '.', 'Writing', 'code', 'was', 'somewhat', 'confusing', 'because', 'I', 'didn', '&', '#', '039', ';', 't', 'know', 'java', 'before', '.', 'Actually', 'teaching', 'is', 'very', 'good', 'and', 'can', 'understand', 'easily', 'the', 'concepts', 'by', 'examples', 'which', 'are', 'given', 'in', 'the', 'class.it', 'will', 'be', 'more', 'helpful', 'if', 'provide', 'solved', 'questions', 'as', 'well', '!', '.', 'thankyou']\n"
     ]
    }
   ],
   "source": [
    "#read student_feedback.txt file\n",
    "feedbackFile = open(\"C:\\\\Users\\\\Supun\\\\Desktop\\\\Supun Backup\\\\Supun\\\\transfer\\\\documents\\\\myfiles\\\\MScDA\\\\DS\\\\IR\\\\Assignment_1\\\\student_feedback.txt\", \"r\", encoding='utf-8')\n",
    "feedbackdata = feedbackFile.read()\n",
    "\n",
    "#tokenize\n",
    "token_feedback = nltk.word_tokenize(feedbackdata)\n",
    "\n",
    "print(token_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Research paper data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promising', 'opportunities', 'for', 'multi-task', 'learning', ',', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'task-invariant', 'features', '.', 'However', ',', 'in', 'most', 'existing', 'approaches', ',', 'the', 'extracted', 'shared', 'features', 'are', 'prone', 'to', 'be', 'contaminated', 'by', 'task-specific', 'features', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', '.', 'In', 'this', 'paper', ',', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'learning', 'framework', ',', 'alleviating', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interfering', 'with', 'each', 'other', '.', 'We', 'conduct', 'extensive', 'experiments', 'on', '16', 'different', 'text', 'classification', 'tasks', ',', 'which', 'demonstrates', 'the', 'benefits', 'of', 'our', 'approach', '.', 'Besides', ',', 'we', 'show', 'that', 'the', 'shared', 'knowledge', 'learned', 'by', 'our', 'proposed', 'model', 'can', 'be', 'regarded', 'as', 'off-the-shelf', 'knowledge', 'and', 'easily', 'transferred', 'to', 'new', 'tasks', '.', 'Multi-task', 'learning', 'is', 'an', 'effective', 'approach', 'to', 'improve', 'the', 'performance', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', '.', 'Recently', ',', 'neural-based', 'models', 'for', 'multi-task', 'learning', 'have', 'become', 'very', 'popular', ',', 'ranging', 'from', 'computer', 'vision', '(', 'Misra', 'et', 'al.', ',', '2016', ';', 'Zhang', 'et', 'al.', ',', '2014', ')', 'to', 'natural', 'language', 'processing', '(', 'Collobert', 'andWeston', ',', '2008', ';', 'Luong', 'et', 'al.', ',', '2015', ')', ',', 'since', 'they', 'provide', 'a', 'convenient', 'way', 'of', 'combining', 'information', 'from', 'multiple', 'tasks', '.', 'However', ',', 'most', 'existing', 'work', 'on', 'multi-task', 'learning', '(', 'Liu', 'et', 'al.', ',', '2016c', ',', 'b', ')', 'attempts', 'to', 'divide', 'the', 'features', 'of', 'different', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', ',', 'merely', 'based', 'on', 'whether', 'parameters', 'of', 'some', 'components', 'should', 'be', 'shared', '.', 'As', 'shown', 'in', 'Figure', '1-', '(', 'a', ')', ',', 'the', 'general', 'shared-private', 'model', 'introduces', 'two', 'feature', 'spaces', 'for', 'any', 'task', ':', 'one', 'is', 'used', 'to', 'store', 'task-dependent', 'features', ',', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'features', '.', 'The', 'major', 'limitation', 'of', 'this', 'framework', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnecessary', 'task-specific', 'features', ',', 'while', 'some', 'sharable', 'features', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', ',', 'suffering', 'from', 'feature', 'redundancy', '.', 'Taking', 'the', 'following', 'two', 'sentences', 'as', 'examples', ',', 'which', 'are', 'extracted', 'from', 'two', 'different', 'sentiment', 'classification', 'tasks', ':', 'Movie', 'reviews', 'and', 'Baby', 'products', 'reviews', '.', 'The', 'infantile', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', '.', 'This', 'kind', 'of', 'humour', 'is', 'infantile', 'and', 'boring', '.', 'The', 'word', '�infantile�', 'indicates', 'negative', 'sentiment', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', '.', 'However', ',', 'the', 'general', 'shared-private', 'model', 'could', 'place', 'the', 'task-specific', 'word', '�infantile�', 'in', 'a', 'shared', 'space', ',', 'leaving', 'potential', 'hazards', 'for', 'other', 'tasks', '.', 'Additionally', ',', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnecessary', 'features', '.', 'To', 'address', 'this', 'problem', ',', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversarial', 'multi-task', 'framework', ',', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'herently', 'disjoint', 'by', 'introducing', 'orthogonality', 'constraints.Specifically', ',', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'framework', 'to', 'model', 'the', 'text', 'sequence', '.']\n"
     ]
    }
   ],
   "source": [
    "#read research_paper.txt file\n",
    "researchFile = open(\"C:\\\\Users\\\\Supun\\\\Desktop\\\\Supun Backup\\\\Supun\\\\transfer\\\\documents\\\\myfiles\\\\MScDA\\\\DS\\\\IR\\\\Assignment_1\\\\research_paper.txt\", \"r\", encoding='utf-8')\n",
    "researchdata = researchFile.read()\n",
    "\n",
    "#tokenize\n",
    "token_research = nltk.word_tokenize(researchdata)\n",
    "\n",
    "print(token_research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolated spell correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspellchecker in c:\\users\\supun\\anaconda3\\lib\\site-packages (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker(distance=2)\n",
    "\n",
    "\n",
    "def wordCorrection(tokenSet):\n",
    "    # find those words that may be misspelled\n",
    "    misspelled = spell.unknown(tokenSet)\n",
    "\n",
    "    for word in misspelled:\n",
    "              \n",
    "        print(\"{} -> mostly liked answer : {} \".format(word, spell.correction(word)))\n",
    "        print(\"Mostly Likely Options : {} \".format(spell.candidates(word)))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uvre -> mostly liked answer : u're \n",
      "Mostly Likely Options : {\"u're\", 'uver', 'ure'} \n",
      "\n",
      "\n",
      "https://t.co/f4uto5a7zf -> mostly liked answer : https://t.co/f4uto5a7zf \n",
      "Mostly Likely Options : {'https://t.co/f4uto5a7zf'} \n",
      "\n",
      "\n",
      "... -> mostly liked answer : ... \n",
      "Mostly Likely Options : {'...'} \n",
      "\n",
      "\n",
      "https://t.co/m5ckgyvv8f -> mostly liked answer : https://t.co/m5ckgyvv8f \n",
      "Mostly Likely Options : {'https://t.co/m5ckgyvv8f'} \n",
      "\n",
      "\n",
      "https://t.co/99myliuoes -> mostly liked answer : https://t.co/99myliuoes \n",
      "Mostly Likely Options : {'https://t.co/99myliuoes'} \n",
      "\n",
      "\n",
      "https://t.co/uvruw8er1b -> mostly liked answer : https://t.co/uvruw8er1b \n",
      "Mostly Likely Options : {'https://t.co/uvruw8er1b'} \n",
      "\n",
      "\n",
      "https://t.co/xxdeig7dbu -> mostly liked answer : https://t.co/xxdeig7dbu \n",
      "Mostly Likely Options : {'https://t.co/xxdeig7dbu'} \n",
      "\n",
      "\n",
      "d -> mostly liked answer : i \n",
      "Mostly Likely Options : {'dy', 'y', 'da', 'yd', 'o', 'ad', 'a', 'id', 'ud', 'i', 'de', 'od', 'du', 'do', 'ed', 'e', 'u', 'di'} \n",
      "\n",
      "\n",
      "#racism -> mostly liked answer : racism \n",
      "Mostly Likely Options : {'racism'} \n",
      "\n",
      "\n",
      "@huffingtonpost -> mostly liked answer : @huffingtonpost \n",
      "Mostly Likely Options : {'@huffingtonpost'} \n",
      "\n",
      "\n",
      "https://t.co/yxh5w53sro -> mostly liked answer : https://t.co/yxh5w53sro \n",
      "Mostly Likely Options : {'https://t.co/yxh5w53sro'} \n",
      "\n",
      "\n",
      "d'immigration -> mostly liked answer : immigration \n",
      "Mostly Likely Options : {'immigration'} \n",
      "\n",
      "\n",
      "https://t.co/kwz3csvyxm -> mostly liked answer : https://t.co/kwz3csvyxm \n",
      "Mostly Likely Options : {'https://t.co/kwz3csvyxm'} \n",
      "\n",
      "\n",
      "m -> mostly liked answer : i \n",
      "Mostly Likely Options : {'am', 'mu', 'y', 'o', 'om', 'a', 'i', 'mi', 'um', 'my', 'em', 'me', 'ma', 'mo', 'e', 'u', 'im', 'ym'} \n",
      "\n",
      "\n",
      "b -> mostly liked answer : i \n",
      "Mostly Likely Options : {'y', 'o', 'ba', 'a', 'ob', 'ub', 'i', 'bi', 'eb', 'ab', 'bo', 'by', 'ib', 'be', 'bu', 'e', 'u'} \n",
      "\n",
      "\n",
      "xkofy -> mostly liked answer : goofy \n",
      "Mostly Likely Options : {'ofy', 'doofy', 'foofy', 'goofy', 'poofy', 'kofi'} \n",
      "\n",
      "\n",
      "#statistics -> mostly liked answer : statistics \n",
      "Mostly Likely Options : {'statistics'} \n",
      "\n",
      "\n",
      "@youtube -> mostly liked answer : youtube \n",
      "Mostly Likely Options : {'youtube'} \n",
      "\n",
      "\n",
      "haryanavi -> mostly liked answer : haryanavi \n",
      "Mostly Likely Options : {'haryanavi'} \n",
      "\n",
      "\n",
      "https://t.co/ihpvhw2bag -> mostly liked answer : https://t.co/ihpvhw2bag \n",
      "Mostly Likely Options : {'https://t.co/ihpvhw2bag'} \n",
      "\n",
      "\n",
      "https://t.co/cneywn40x3 -> mostly liked answer : https://t.co/cneywn40x3 \n",
      "Mostly Likely Options : {'https://t.co/cneywn40x3'} \n",
      "\n",
      "\n",
      "https://t.co/lsg7c3vle9 -> mostly liked answer : https://t.co/lsg7c3vle9 \n",
      "Mostly Likely Options : {'https://t.co/lsg7c3vle9'} \n",
      "\n",
      "\n",
      "https://t.co/cyi867pzrv -> mostly liked answer : https://t.co/cyi867pzrv \n",
      "Mostly Likely Options : {'https://t.co/cyi867pzrv'} \n",
      "\n",
      "\n",
      "#immigrants -> mostly liked answer : immigrants \n",
      "Mostly Likely Options : {'immigrants'} \n",
      "\n",
      "\n",
      "#visa -> mostly liked answer : visa \n",
      "Mostly Likely Options : {'visa'} \n",
      "\n",
      "\n",
      "vaste -> mostly liked answer : waste \n",
      "Mostly Likely Options : {'haste', 'caste', 'vashe', 'vasty', 'viste', 'maste', 'vase', 'vast', 'baste', 'paste', 'vaster', 'taste', 'waste'} \n",
      "\n",
      "\n",
      "visa-free -> mostly liked answer : visa-free \n",
      "Mostly Likely Options : {'visa-free'} \n",
      "\n",
      "\n",
      "https://t.co/wtqk4qdiki -> mostly liked answer : https://t.co/wtqk4qdiki \n",
      "Mostly Likely Options : {'https://t.co/wtqk4qdiki'} \n",
      "\n",
      "\n",
      "https://t.co/uibsezoqas -> mostly liked answer : https://t.co/uibsezoqas \n",
      "Mostly Likely Options : {'https://t.co/uibsezoqas'} \n",
      "\n",
      "\n",
      "https://t.co/vcmfygadr5 -> mostly liked answer : https://t.co/vcmfygadr5 \n",
      "Mostly Likely Options : {'https://t.co/vcmfygadr5'} \n",
      "\n",
      "\n",
      "guli -> mostly liked answer : gulf \n",
      "Mostly Likely Options : {'gui', 'goli', 'guil', 'gusli', 'guni', 'gull', 'juli', 'gul', 'gli', 'gali', 'gulp', 'uli', 'gulf'} \n",
      "\n",
      "\n",
      "#canada -> mostly liked answer : canada \n",
      "Mostly Likely Options : {'canada'} \n",
      "\n",
      "\n",
      "@ladymadonna___ -> mostly liked answer : @ladymadonna___ \n",
      "Mostly Likely Options : {'@ladymadonna___'} \n",
      "\n",
      "\n",
      "cortiqu -> mostly liked answer : cortiqu \n",
      "Mostly Likely Options : {'cortiqu'} \n",
      "\n",
      "\n",
      "https://t.co/becgusy2i6 -> mostly liked answer : https://t.co/becgusy2i6 \n",
      "Mostly Likely Options : {'https://t.co/becgusy2i6'} \n",
      "\n",
      "\n",
      "https://t.co/s3hu1okkig -> mostly liked answer : https://t.co/s3hu1okkig \n",
      "Mostly Likely Options : {'https://t.co/s3hu1okkig'} \n",
      "\n",
      "\n",
      "l -> mostly liked answer : i \n",
      "Mostly Likely Options : {'yl', 'y', 'el', 'la', 'o', 'lo', 'a', 'li', 'i', 'ly', 'ol', 'il', 'al', 'ul', 'lu', 'e', 'u', 'le'} \n",
      "\n",
      "\n",
      "l'immigration -> mostly liked answer : immigration \n",
      "Mostly Likely Options : {'lmmigration', 'immigration'} \n",
      "\n",
      "\n",
      "https://t.co/5lievho7a4 -> mostly liked answer : https://t.co/5lievho7a4 \n",
      "Mostly Likely Options : {'https://t.co/5lievho7a4'} \n",
      "\n",
      "\n",
      "@shawhelp -> mostly liked answer : @shawhelp \n",
      "Mostly Likely Options : {'@shawhelp'} \n",
      "\n",
      "\n",
      "@sweetnessshawnb -> mostly liked answer : @sweetnessshawnb \n",
      "Mostly Likely Options : {'@sweetnessshawnb'} \n",
      "\n",
      "\n",
      "https://t.co/yagwmz8ecp -> mostly liked answer : https://t.co/yagwmz8ecp \n",
      "Mostly Likely Options : {'https://t.co/yagwmz8ecp'} \n",
      "\n",
      "\n",
      "#cpcldr -> mostly liked answer : #cpcldr \n",
      "Mostly Likely Options : {'#cpcldr'} \n",
      "\n",
      "\n",
      "� -> mostly liked answer : i \n",
      "Mostly Likely Options : {'y', 'o', 'a', 'i', 'e', 'u'} \n",
      "\n",
      "\n",
      "=p -> mostly liked answer : up \n",
      "Mostly Likely Options : {'ip', 'up', 'ap', 'op', 'ep'} \n",
      "\n",
      "\n",
      "toft -> mostly liked answer : soft \n",
      "Mostly Likely Options : {'toff', 'oft', 'toot', 'loft', 'tout', \"to't\", 'taft', 'tuft', 'tort', 'toet', 'soft', 'tofu', 'tot'} \n",
      "\n",
      "\n",
      "abhinav -> mostly liked answer : abhinav \n",
      "Mostly Likely Options : {'abhinav'} \n",
      "\n",
      "\n",
      "#hatecrime -> mostly liked answer : #hatecrime \n",
      "Mostly Likely Options : {'#hatecrime'} \n",
      "\n",
      "\n",
      "mr -> mostly liked answer : my \n",
      "Mostly Likely Options : {'mu', 'my', 'me', 'mar', 'or', 'mur', 'yr', 'ar', 'ma', 'mi', 'mre', 'ur', 'ir', 'mo', 'mri', 'mir', 'er', 'mer', 'mor'} \n",
      "\n",
      "\n",
      "https://t.co/zozose1cqq -> mostly liked answer : https://t.co/zozose1cqq \n",
      "Mostly Likely Options : {'https://t.co/zozose1cqq'} \n",
      "\n",
      "\n",
      "https://t.co/ec3xhoro2s -> mostly liked answer : https://t.co/ec3xhoro2s \n",
      "Mostly Likely Options : {'https://t.co/ec3xhoro2s'} \n",
      "\n",
      "\n",
      "https://t.co/nefw30mraa -> mostly liked answer : https://t.co/nefw30mraa \n",
      "Mostly Likely Options : {'https://t.co/nefw30mraa'} \n",
      "\n",
      "\n",
      "https://t.co/rqrr5nebcg -> mostly liked answer : https://t.co/rqrr5nebcg \n",
      "Mostly Likely Options : {'https://t.co/rqrr5nebcg'} \n",
      "\n",
      "\n",
      "st -> mostly liked answer : it \n",
      "Mostly Likely Options : {'sot', 'sit', 'se', 'ist', 'ot', 'sut', 'set', 'yst', 'ste', 'sat', 'stu', 'it', 'so', 'sto', 'ast', 'ut', 'ust', 'sta', 'at', 'su', 'si', 'ost', 'sti', 'est', 'sy', 'sty', 'et', 'sa'} \n",
      "\n",
      "\n",
      "https://t.co/dkpukywmak -> mostly liked answer : https://t.co/dkpukywmak \n",
      "Mostly Likely Options : {'https://t.co/dkpukywmak'} \n",
      "\n",
      "\n",
      "#onthisday -> mostly liked answer : #onthisday \n",
      "Mostly Likely Options : {'#onthisday'} \n",
      "\n",
      "\n",
      "@theeconomist -> mostly liked answer : @theeconomist \n",
      "Mostly Likely Options : {'@theeconomist'} \n",
      "\n",
      "\n",
      "deyfy -> mostly liked answer : defy \n",
      "Mostly Likely Options : {'deify', 'defy'} \n",
      "\n",
      "\n",
      "irr -> mostly liked answer : err \n",
      "Mostly Likely Options : {'ier', 'iru', 'ira', 'irl', 'orr', 'ihr', 'irn', 'irt', 'inr', 'isr', 'ior', 'irh', 'ire', 'ir', 'ifr', 'irk', 'irs', 'err', 'arr', 'iar', 'iro', 'irv'} \n",
      "\n",
      "\n",
      "#sitetraffic -> mostly liked answer : #sitetraffic \n",
      "Mostly Likely Options : {'#sitetraffic'} \n",
      "\n",
      "\n",
      "https://t.co/0c5obfmxlg -> mostly liked answer : https://t.co/0c5obfmxlg \n",
      "Mostly Likely Options : {'https://t.co/0c5obfmxlg'} \n",
      "\n",
      "\n",
      "#fasttraffic -> mostly liked answer : #fasttraffic \n",
      "Mostly Likely Options : {'#fasttraffic'} \n",
      "\n",
      "\n",
      "https://t.co/j77devjoix -> mostly liked answer : https://t.co/j77devjoix \n",
      "Mostly Likely Options : {'https://t.co/j77devjoix'} \n",
      "\n",
      "\n",
      "manitoba -> mostly liked answer : manitou \n",
      "Mostly Likely Options : {'manitou'} \n",
      "\n",
      "\n",
      "#usa -> mostly liked answer : usa \n",
      "Mostly Likely Options : {'usa', 'ausa', 'musa', 'busa'} \n",
      "\n",
      "\n",
      "s -> mostly liked answer : i \n",
      "Mostly Likely Options : {'sa', 'y', 'o', 'a', 'os', 'i', 'so', 'es', 'ys', 'se', 'sy', 'u', 'e', 'su', 'si', 'us', 'as', 'is'} \n",
      "\n",
      "\n",
      "https://t.co/9i72frhtij -> mostly liked answer : https://t.co/9i72frhtij \n",
      "Mostly Likely Options : {'https://t.co/9i72frhtij'} \n",
      "\n",
      "\n",
      "262m -> mostly liked answer : 262m \n",
      "Mostly Likely Options : {'262m'} \n",
      "\n",
      "\n",
      "fear-mongering -> mostly liked answer : fearmongering \n",
      "Mostly Likely Options : {'fearmongering'} \n",
      "\n",
      "\n",
      "#lpc -> mostly liked answer : bloc \n",
      "Mostly Likely Options : {'lpo', 'luc', 'loc', 'bloc', 'alc', 'plac', 'olnc', 'blic', 'alp', 'flic', 'lac', 'elp', 'ipc', 'alps', 'apc', 'alec'} \n",
      "\n",
      "\n",
      "monsef -> mostly liked answer : money \n",
      "Mostly Likely Options : {'moses', 'onset', 'monde', 'mons', 'monger', 'yousef', 'monster', 'menses', 'mouser', 'moose', 'monev', 'monies', 'munsey', 'money', 'mone', 'morse', 'mooses', 'mouse', 'manse', 'monsta', 'mouses', 'onsen', 'morsel', 'monte', 'mose', 'josef', 'yosef', 'montez', 'montes', 'consec', 'moser', 'monkey', 'mosey', 'moosey', 'monet', 'mousey', 'moises', 'mosses'} \n",
      "\n",
      "\n",
      "#greencard -> mostly liked answer : #greencard \n",
      "Mostly Likely Options : {'#greencard'} \n",
      "\n",
      "\n",
      "https://t.co/zrlj26jnkc -> mostly liked answer : https://t.co/zrlj26jnkc \n",
      "Mostly Likely Options : {'https://t.co/zrlj26jnkc'} \n",
      "\n",
      "\n",
      "https://t.co/kxdfmgtzzn -> mostly liked answer : https://t.co/kxdfmgtzzn \n",
      "Mostly Likely Options : {'https://t.co/kxdfmgtzzn'} \n",
      "\n",
      "\n",
      "campagne -> mostly liked answer : champagne \n",
      "Mostly Likely Options : {'campagnes', 'champagne'} \n",
      "\n",
      "\n",
      "know-all -> mostly liked answer : knowall \n",
      "Mostly Likely Options : {'knowall'} \n",
      "\n",
      "\n",
      "#cdnpoli -> mostly liked answer : #cdnpoli \n",
      "Mostly Likely Options : {'#cdnpoli'} \n",
      "\n",
      "\n",
      "https://t.co/4k84ee8y63 -> mostly liked answer : https://t.co/4k84ee8y63 \n",
      "Mostly Likely Options : {'https://t.co/4k84ee8y63'} \n",
      "\n",
      "\n",
      "#immigration -> mostly liked answer : immigration \n",
      "Mostly Likely Options : {'immigration'} \n",
      "\n",
      "\n",
      "#traffic -> mostly liked answer : traffic \n",
      "Mostly Likely Options : {'traffic'} \n",
      "\n",
      "\n",
      "#jamaican -> mostly liked answer : jamaican \n",
      "Mostly Likely Options : {'jamaican'} \n",
      "\n",
      "\n",
      "@canadidly -> mostly liked answer : candidly \n",
      "Mostly Likely Options : {'candidly'} \n",
      "\n",
      "\n",
      "c -> mostly liked answer : i \n",
      "Mostly Likely Options : {'uc', 'y', 'ec', 'ic', 'ci', 'o', 'cy', 'ce', 'co', 'i', 'ca', 'a', 'ac', 'cu', 'oc', 'e', 'u'} \n",
      "\n",
      "\n",
      "https://t.co/x2ifo0exi2 -> mostly liked answer : https://t.co/x2ifo0exi2 \n",
      "Mostly Likely Options : {'https://t.co/x2ifo0exi2'} \n",
      "\n",
      "\n",
      "rigoureusement -> mostly liked answer : rigoureusement \n",
      "Mostly Likely Options : {'rigoureusement'} \n",
      "\n",
      "\n",
      "#integration -> mostly liked answer : integration \n",
      "Mostly Likely Options : {'integration'} \n",
      "\n",
      "\n",
      "#website -> mostly liked answer : website \n",
      "Mostly Likely Options : {'website'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#twitter data\n",
    "wordCorrection(token_twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. -> mostly liked answer : i \n",
      "Mostly Likely Options : {'yn', 'am', 'y', 'ec', 'xa', 'yl', 'ip', 'ho', 'ub', 'bu', 'ac', 'zi', 'he', 'du', 'we', 'vi', 'ot', 'ik', 'ox', 'eg', 'ei', 'ci', 'qi', 'eo', 'ku', 'av', 'i', 'uy', 'eb', 'yw', 'ak', 'ud', 'ui', 'ij', 'jy', 'xe', 'ma', 'uv', 'go', 'oo', 'va', 'gi', 'ke', 'mi', 'au', 'ys', 'ev', 'ol', 'if', 'zo', 'ne', 'do', 'na', 'si', 'us', 'ru', 'wa', 'gy', 'o', 'ay', 'af', 'pa', 'za', 'qo', 'um', 'bo', 'ey', 'fu', 'ly', 'wu', 'ea', 'ae', 'ao', 'po', 'io', 'yg', 'ep', 'ro', 'eu', 'to', 'ga', 'ob', 'tu', 'fa', 'an', 'se', 'ej', 'me', 'pi', 'or', 'ua', 'ef', 'lu', 'ja', 'ka', 'ha', 'ex', 'no', 'iu', 'cy', 'ia', 'so', 'up', 'wo', 'in', 'ai', 'yu', 'il', 'ri', 'hy', 'ut', 'wi', 'iy', 'ad', 'ji', 'az', 'je', 'de', 'vo', 'es', 'ur', 'ag', 'ie', 'by', 'di', 'dy', 'oj', 'ic', 'om', 'bi', 'ca', 'oh', 'gu', 'ok', 'sy', 'uf', 'qa', 'ta', 'xi', 'er', 'ue', 'of', 'ed', 'fi', 'ek', 'xo', 'et', 'nu', 'yo', 'mu', 'ii', 'uo', 'ye', 'eq', 'on', 'wy', 'li', 'yd', 'ew', 'uh', 'my', 'em', 'og', 'zu', 'op', 're', 'ba', 'u', 'le', 'hi', 'uj', 'el', 'ze', 've', 'ce', 'it', 'ih', 'uw', 'oa', 'lo', 'ee', 'eh', 'co', 'ov', 'ig', 'vy', 'ir', 'qe', 'ib', 'oc', 'at', 'mo', 'su', 'ul', 'xu', 'ix', 'fy', 'la', 'a', 'ug', 'id', 'ny', 'jo', 'yi', 'al', 'uk', 'ko', 'ty', 'en', 'hu', 'od', 'oy', 'ap', 'ow', 'qu', 'os', 'ax', 'ez', 'pe', 'yr', 'ge', 'oi', 'ar', 'be', 'ra', 'ya', 'oe', 'fo', 'iq', 'uc', 'iz', 'pu', 'ki', 'aq', 'vu', 'iv', 'fe', 'xy', 'oz', 'ti', 'ou', 'cu', 'ry', 'e', 'as', 'un', 'ym', 'aj', 'te', 'da', 'ni', 'ah', 'ju', 'ab', 'ky', 'py', 'aw', 'sa', 'im', 'is'} \n",
      "\n",
      "\n",
      "lf -> mostly liked answer : of \n",
      "Mostly Likely Options : {'li', 'elf', 'lef', 'lu', 'ef', 'le', 'lof', 'lif', 'alf', 'lo', 'if', 'la', 'af', 'uf', 'ulf', 'lfi', 'ly', 'of', 'lfe'} \n",
      "\n",
      "\n",
      "self-study -> mostly liked answer : self-study \n",
      "Mostly Likely Options : {'self-study'} \n",
      "\n",
      "\n",
      "helpfull -> mostly liked answer : helpful \n",
      "Mostly Likely Options : {'helpful', 'helpfully'} \n",
      "\n",
      "\n",
      "exercisers -> mostly liked answer : exercises \n",
      "Mostly Likely Options : {'exercises'} \n",
      "\n",
      "\n",
      "'' -> mostly liked answer : i \n",
      "Mostly Likely Options : {'yl', 'am', 'y', 'ec', 'xa', 'yn', 'ip', 'ho', 'ub', 'ac', 'zi', 'he', 'du', 'we', 'vi', 'ot', 'ox', 'ik', 'eg', 'uy', \"e's\", 'ku', 'ua', 'av', 'ei', 'eo', \"u'd\", 'eb', 'ak', 'yw', 'ud', 'ui', 'i', 'ij', 'xe', 'jy', 'ma', 'uv', 'go', 'oo', 'va', \"j'y\", 'ke', \"c'a\", 'gi', \"i'a\", 'mi', 'au', 'ys', 'ol', 'ev', 'if', 'zo', 'ne', 'do', \"i'i\", 'na', 'si', 'us', 'ru', 'wa', 'gy', 'o', 'ay', 'af', 'pa', 'za', 'qo', 'um', 'bo', 'ly', 'ey', \"e'd\", 'wu', 'ea', 'fu', 'ae', 'ao', \"i'm\", \"n'e\", 'po', 'io', 'yg', 'ep', 'ro', 'ga', 'eu', 'to', 'ob', 'tu', 'fa', 'aw', 'se', 'an', 'me', 'ej', 'pi', 'or', 'lu', 'ef', 'ja', 'ka', 'ha', \"o'c\", 'ex', 'qi', 'no', 'ia', 'cy', 'iu', 'up', 'so', \"y'd\", 'wo', 'ri', 'in', 'ai', 'il', 'yu', 'hy', \"y'a\", 'ut', 'wi', 'iy', 'ad', 'ji', 'az', \"m'e\", 'je', 'de', 'vo', 'es', 'ur', 'ag', 'ie', 'by', \"m'a\", 'di', 'dy', 'oj', 'ic', 'om', \"d'a\", 'bi', 'ca', \"n'y\", 'oh', 'ok', 'sy', 'gu', 'ta', 'ue', 'qa', 'er', 'xi', 'of', 'ed', 'fi', 'ek', 'xo', 'et', 'nu', 'yo', \"i'n\", 'mu', 'uo', 'ii', 'ye', 'on', 'wy', \"y's\", 'li', 'eq', \"u's\", 'ew', 'uh', 'yd', 'my', \"c'e\", 'em', 'og', 'zu', 'op', 're', 'ba', 'le', 'u', 'uj', 'hi', 'el', 'ze', 've', 'ce', 'it', 'ih', 'uw', \"i's\", 'oa', 'lo', 'ee', \"n'a\", \"i'd\", 'eh', 'co', \"i'l\", 'ov', 'ig', 'vy', 'ir', 'qe', 'ib', 'oc', 'at', 'mo', 'su', 'ul', 'ix', 'xu', 'fy', 'la', 'a', 'ug', 'id', 'ny', 'yi', 'jo', 'al', 'uf', 'uk', 'ko', 'ty', 'od', 'en', 'hu', 'oy', 'ap', 'ow', 'qu', 'os', 'ax', 'ez', 'pe', 'yr', 'ge', 'oi', 'ar', 'be', 'ra', 'ya', 'oe', 'fo', 'iq', 'uc', 'ki', 'pu', 'iz', \"m'y\", 'iv', 'vu', 'aq', 'fe', 'xy', 'oz', 'ou', 'ti', 'cu', 'ry', \"o's\", 'e', 'ci', 'as', 'un', 'ym', 'da', 'te', \"i'v\", 'ni', \"i't\", 'aj', 'ah', 'ju', 'ab', 'ky', 'py', \"y'v\", 'bu', 'sa', 'im', 'is'} \n",
      "\n",
      "\n",
      "madame. -> mostly liked answer : madame \n",
      "Mostly Likely Options : {'madames', 'madame'} \n",
      "\n",
      "\n",
      "undersatand -> mostly liked answer : understand \n",
      "Mostly Likely Options : {'understand'} \n",
      "\n",
      "\n",
      "examples.lectures -> mostly liked answer : examples.lectures \n",
      "Mostly Likely Options : {'examples.lectures'} \n",
      "\n",
      "\n",
      "speed.a -> mostly liked answer : speed \n",
      "Mostly Likely Options : {'speed', 'speedee', 'speedos', 'spenda', 'speeder', 'speedo', 'speeded', 'speedway', 'speedoo', \"speed's\", 'speeds', 'speedy'} \n",
      "\n",
      "\n",
      "interesting.we -> mostly liked answer : interesting.we \n",
      "Mostly Likely Options : {'interesting.we'} \n",
      "\n",
      "\n",
      "lectuers -> mostly liked answer : lectures \n",
      "Mostly Likely Options : {'lectures', 'lecturers'} \n",
      "\n",
      "\n",
      "t -> mostly liked answer : i \n",
      "Mostly Likely Options : {'y', 'te', 'o', 'to', 'a', 'ty', 'it', 'i', 'tu', 'et', 'ta', 'ti', 'u', 'e', 'at', 'ot', 'ut'} \n",
      "\n",
      "\n",
      "class.it -> mostly liked answer : classic \n",
      "Mostly Likely Options : {'classic', 'classist'} \n",
      "\n",
      "\n",
      "one.so -> mostly liked answer : one's \n",
      "Mostly Likely Options : {'onewho', 'ones', 'oneto', 'oneis', 'onesho', \"one's\"} \n",
      "\n",
      "\n",
      "`` -> mostly liked answer : i \n",
      "Mostly Likely Options : {'yl', 'yn', 'y', 'ec', 'am', 'xa', 'ho', 'ip', 'ub', 'ac', 'zi', 'he', 'du', 'we', 'vi', 'ot', 'ik', 'ox', 'eg', 'ku', 'uy', 'ei', 'eo', 'ua', 'ud', 'i', 'ui', 'ak', 'eb', 'ci', 'av', 'yw', 'ij', 'jy', 'xe', 'ma', 'go', 'uv', 'oo', 'va', 'gi', 'ke', 'mi', 'au', 'ys', 'ol', 'ev', 'if', 'zo', 'ne', 'do', 'na', 'si', 'us', 'ru', 'wa', 'gy', 'o', 'ay', 'af', 'pa', 'za', 'qo', 'um', 'bo', 'ly', 'ey', 'fu', 'wu', 'ea', 'ao', 'ae', 'po', 'io', 'yg', 'ep', 'ro', 'to', 'eu', 'ga', 'ob', 'tu', 'fa', 'an', 'se', 'aw', 'me', 'ej', 'pi', 'or', 'ef', 'lu', 'ja', 'ka', 'ha', 'ex', 'qi', 'no', 'iu', 'cy', 'ia', 'so', 'up', 'wo', 'ri', 'in', 'yu', 'il', 'ai', 'hy', 'ut', 'wi', 'iy', 'ad', 'ji', 'az', 'je', 'de', 'vo', 'es', 'ur', 'ag', 'ie', 'by', 'di', 'dy', 'oj', 'ic', 'om', 'bi', 'uf', 'ca', 'gu', 'ok', 'sy', 'oh', 'ta', 'ue', 'xi', 'er', 'qa', 'of', 'xo', 'ed', 'ek', 'fi', 'et', 'nu', 'yo', 'mu', 'uo', 'ii', 'ye', 'wy', 'eq', 'on', 'li', 'yd', 'ew', 'uh', 'my', 'em', 'og', 'zu', 'op', 're', 'u', 'uj', 'ba', 'hi', 'le', 'el', 'ze', 've', 'ce', 'it', 'ih', 'uw', 'oa', 'lo', 'ee', 'eh', 'co', 'ov', 'ig', 'vy', 'ir', 'qe', 'ib', 'oc', 'at', 'mo', 'su', 'ul', 'ix', 'xu', 'fy', 'la', 'a', 'ug', 'id', 'ny', 'jo', 'yi', 'al', 'uk', 'ko', 'ty', 'en', 'hu', 'od', 'oy', 'ap', 'ow', 'qu', 'ax', 'os', 'ez', 'pe', 'yr', 'ge', 'oi', 'ar', 'be', 'ra', 'ya', 'fo', 'oe', 'iq', 'uc', 'ki', 'pu', 'iz', 'aq', 'vu', 'iv', 'xy', 'fe', 'oz', 'ti', 'ou', 'cu', 'ry', 'e', 'as', 'un', 'ym', 'aj', 'te', 'ni', 'da', 'ah', 'ju', 'ab', 'ky', 'py', 'bu', 'sa', 'im', 'is'} \n",
      "\n",
      "\n",
      "s -> mostly liked answer : i \n",
      "Mostly Likely Options : {'sa', 'y', 'o', 'a', 'os', 'i', 'so', 'es', 'ys', 'se', 'sy', 'u', 'e', 'su', 'si', 'us', 'as', 'is'} \n",
      "\n",
      "\n",
      "br -> mostly liked answer : be \n",
      "Mostly Likely Options : {'bor', 'bra', 'or', 'ba', 'yr', 'bri', 'ar', 'be', 'ur', 'ir', 'bur', 'by', 'bi', 'bre', 'bo', 'bir', 'er', 'bu', 'ber', 'bar', 'bro'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Student feedback data\n",
    "wordCorrection(token_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constraints.specifically -> mostly liked answer : constraints.specifically \n",
      "Mostly Likely Options : {'constraints.specifically'} \n",
      "\n",
      "\n",
      "task-dependent -> mostly liked answer : task-dependent \n",
      "Mostly Likely Options : {'task-dependent'} \n",
      "\n",
      "\n",
      "luong -> mostly liked answer : long \n",
      "Mostly Likely Options : {'ulong', 'long', 'lung', 'loong'} \n",
      "\n",
      "\n",
      "task-invariant -> mostly liked answer : task-invariant \n",
      "Mostly Likely Options : {'task-invariant'} \n",
      "\n",
      "\n",
      "�infantile� -> mostly liked answer : infantile \n",
      "Mostly Likely Options : {'infantile'} \n",
      "\n",
      "\n",
      "shared-private -> mostly liked answer : shared-private \n",
      "Mostly Likely Options : {'shared-private'} \n",
      "\n",
      "\n",
      "off-the-shelf -> mostly liked answer : off-the-shelf \n",
      "Mostly Likely Options : {'off-the-shelf'} \n",
      "\n",
      "\n",
      "2016c -> mostly liked answer : 2016c \n",
      "Mostly Likely Options : {'2016c'} \n",
      "\n",
      "\n",
      "collobert -> mostly liked answer : colbert \n",
      "Mostly Likely Options : {'colbert'} \n",
      "\n",
      "\n",
      "multi-task -> mostly liked answer : multitask \n",
      "Mostly Likely Options : {'multitask'} \n",
      "\n",
      "\n",
      "misra -> mostly liked answer : mira \n",
      "Mostly Likely Options : {'mista', 'migra', 'masra', 'misha', 'misa', 'misma', 'missa', 'mira'} \n",
      "\n",
      "\n",
      "1- -> mostly liked answer : i \n",
      "Mostly Likely Options : {'am', 'yl', 'yn', 'ec', 'xa', 'y', 'ho', 'ip', 'ub', 'bu', 'ac', 'zi', 'he', 'du', 'we', 'vi', 'ot', 'ox', 'ik', 'qi', 'ei', 'ci', 'ku', 'eg', 'av', 'eo', 'i', 'ui', 'ak', 'eb', 'yw', 'ud', 'ij', 'xe', 'jy', 'ma', 'uv', 'go', 'oo', 'va', 'gi', 'ke', 'mi', 'au', 'ys', 'ol', 'ev', 'if', 'zo', 'ne', 'do', 'na', 'si', 'us', 'ru', 'wa', 'gy', 'ay', 'o', 'af', 'um', 'pa', 'za', 'qo', 'bo', 'ey', 'fu', 'ly', 'wu', 'ea', 'ae', 'ao', 'uy', 'po', 'io', 'ep', 'yg', 'ro', 'eu', 'ga', 'to', 'ob', 'tu', 'fa', 'an', 'se', 'me', 'ej', 'pi', 'ua', 'or', 'ef', 'lu', 'ja', 'ka', 'ha', 'ex', 'no', 'ia', 'cy', 'iu', 'up', 'so', 'wo', 'ai', 'ri', 'in', 'il', 'yu', 'hy', 'ut', 'wi', 'iy', 'ad', 'az', 'ji', 'je', 'de', 'vo', 'es', 'ur', 'ag', 'ie', 'by', 'di', 'dy', 'oj', 'ic', 'om', 'bi', 'uf', 'ca', 'gu', 'sy', 'oh', 'ok', 'ta', 'xi', 'ue', 'er', 'qa', 'of', 'xo', 'fi', 'ed', 'ek', 'et', 'nu', 'yo', 'mu', 'ii', 'uo', 'ye', 'eq', 'on', 'wy', 'li', 'yd', 'ew', 'uh', 'my', 'em', 'og', 'zu', 'op', 're', 'ba', 'hi', 'le', 'u', 'uj', 'el', 'ze', 've', 'ce', 'it', 'ih', 'uw', 'oa', 'lo', 'ee', 'eh', 'co', 'ov', 'ig', 'vy', 'ir', 'qe', 'ib', 'oc', 'at', 'mo', 'su', 'ul', 'xu', 'ix', 'fy', 'la', 'a', 'ug', 'id', 'ny', 'yi', 'jo', 'al', 'uk', 'ko', 'ty', 'hu', 'en', 'od', 'oy', 'ap', 'ow', 'qu', 'ax', 'os', 'ez', 'pe', 'yr', 'ge', 'oi', 'ar', 'be', 'ra', 'ya', 'fo', 'oe', 'iq', 'uc', 'ki', 'pu', 'aq', 'iz', 'iv', 'vu', 'fe', 'xy', 'oz', 'ti', 'ou', 'cu', 'ry', 'e', 'as', 'ym', 'un', 'aj', 'ni', 'te', 'da', 'ah', 'ju', 'ab', 'ky', 'py', 'aw', 'sa', 'im', 'is'} \n",
      "\n",
      "\n",
      "al. -> mostly liked answer : all \n",
      "Mostly Likely Options : {'alk', 'alc', 'alt', 'alg', 'ale', 'alo', 'aly', 'ala', 'alr', 'all', 'al', 'ali', 'als', 'alf', 'alw', 'alp'} \n",
      "\n",
      "\n",
      "herently -> mostly liked answer : recently \n",
      "Mostly Likely Options : {'recently', 'serenely', 'inherently', 'coherently', 'decently', 'fervently'} \n",
      "\n",
      "\n",
      "b -> mostly liked answer : i \n",
      "Mostly Likely Options : {'y', 'o', 'ba', 'a', 'ob', 'ub', 'i', 'bi', 'eb', 'ab', 'bo', 'by', 'ib', 'be', 'bu', 'e', 'u'} \n",
      "\n",
      "\n",
      "orthogonality -> mostly liked answer : orthogonality \n",
      "Mostly Likely Options : {'orthogonality'} \n",
      "\n",
      "\n",
      "task-specific -> mostly liked answer : task-specific \n",
      "Mostly Likely Options : {'task-specific'} \n",
      "\n",
      "\n",
      "neural-based -> mostly liked answer : neural-based \n",
      "Mostly Likely Options : {'neural-based'} \n",
      "\n",
      "\n",
      "andweston -> mostly liked answer : andweston \n",
      "Mostly Likely Options : {'andweston'} \n",
      "\n",
      "\n",
      "sharable -> mostly liked answer : parable \n",
      "Mostly Likely Options : {'scalable', 'shamble', 'salable', 'bearable', 'parable', 'shirabe', 'wearable', 'hirable', 'arable'} \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Research paper data\n",
    "wordCorrection(token_research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Context sensitive word correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\supun\\anaconda3\\lib\\site-packages (from textblob) (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\supun\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.50.2)\n",
      "Requirement already satisfied: click in c:\\users\\supun\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\supun\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\users\\supun\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2020.10.15)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminds me of Liberal Immigration Fraudster Onset avoiding importation from Canada. #cdnpoli #LPC #CPCLDRï¿of¿of http://t.co/ZOZOSe1CqQ\n",
      "#immigration #integration #canada http://t.co/M5cKGyvV8F\n",
      "He want controlled immigration that contributes positively to the of economy. Same as Australia &amp; Canada. http://t.co/99mYliuOes\n",
      "Is the new Manitoba immigration fee a head tax? http://t.co/LsG7C3vLe9\n",
      "Canada immigration profit influence modernistic delhi yet abhinav: XKofy http://t.co/becgusY2i6\n",
      "Canada Immigration Minister to ï¿of¿of¿Substantially Increase  Immigration Numbers http://t.co/nEFw30MRaa http://t.co/cyI867PZRV\n",
      "Of¿of¿me les #USA=pays d'immigration par excellence CONTRï¿of¿½LE RIGOUREUSEMENT l'immigration et act¿of¿is ï¿of¿½ la #GreenCARD!ï¿of¿of http://t.co/IHpVhW2BaG\n",
      "@Shawhelp what changes should be made to Canada's immigration laws due to the influx of immigration and violence?\n",
      "Of¿of¿immigration iron¿of¿½guliï¿of¿are au Canada do¿of¿½cortiquï¿of¿he en 5 questions http://t.co/f4utO5A7ZF\n",
      "L'immigration iron¿of¿½guliï¿of¿are au Canada do¿of¿½cortiquï¿of¿he en 5 questions - http://t.co/UiBsEZOqas http://t.co/j77dEvjoiX http://t.co/XXDeIG7Dbu\n",
      "Will Media ask the Liberals if they actually have a solid plan for Canada of¿of¿½_ï¿of?? From my view -- immigration out of Of¿of¿of http://t.co/YAgwmZ8ECp\n",
      "An Murray of¿of¿Immigration Watch Canada is xenophobic racism fear-wondering liar #racism #canada #cdnpoli #hatecrimeï¿of¿of http://t.co/kwZ3csvYxM\n",
      "He Canada lance une vast champagne d'immigration pour faire face ï¿of¿½ son lesion de main d'ï¿of¿sure http://t.co/kXdfMGTZzN\n",
      "Of¿of¿½#immigration iron¿of¿½guliï¿of¿are au #Canada do¿of¿½cortiquï¿of¿he en of¿of¿questions http://t.co/s3hu1OKKIG\n",
      "@Candidly I've read the Immigration laws of Canada much stricter than the of\n",
      "Canada Immigration Website Traffic Purges And Rashes In Take Of Plump #fasttraffic,#sitetraffic,#webster,#traffic http://t.co/zRlJ26jnkC\n",
      "Or Now-all of Canada Immigration http://t.co/wTQK4QDiKI\n",
      "Love to Canada @LadyMadonna___ Oh, immigration rules, you can't... http://t.co/5LIEVHO7A4\n",
      "#OnThisDay Annette Soft becomes Canada's 2 millionth immigrant since 1945. To you know your family's immigration st¿of¿of http://t.co/UvRuw8eR1b\n",
      ".@TheEconomist profile Canada's open immigration policies &amp; how they contribute to our economic success:ï¿of¿of http://t.co/4K84EE8Y63\n",
      "Hundreds may lose Canadian citizenship, resident status because of one corrupt immigration consultant http://t.co/x2IfO0EXI2\n",
      "Immigration for canada without india: an compassionate handle: delay\n",
      "\"#Jamaica #immigrants #Canada\n",
      "\n",
      "http://t.co/vcmfYGadR5\n",
      "\n",
      "#statistics #immigration\"\n",
      "Mexican isa lift expected to cost Canada $262M over a decade http://t.co/9i72fRhtij\n",
      "Are people still moving to #Canada ??? Oh that's right, they have real immigration laws and it's¿of¿of http://t.co/0C5OBfmxLG\n",
      "Were are more details on the Richmond, B.C. Immigration Consultant Funny Sang who was sentenced to 7 years in... http://t.co/YXH5W53srO\n",
      "I added a video to a @YouTube playlist http://t.co/CnEyWN40x3 Funny Talking of Haryanavi At with Canada Immigration Girl Agent\n",
      "Mexicans An Now Travel Isa-Free To Canada http://t.co/Ec3XHORO2s http://t.co/RQRr5nebcG\n",
      "Of¿of¿immigration iron¿of¿½guliï¿of¿are au Canada do¿of¿½cortiquï¿of¿he en of¿of¿questions http://t.co/DkpuKyWmaK\n",
      "@SweetnessShawnB Yes the POS that cramped up immigration for Canada, among other loyalist policies.\n",
      "Canada lifted isa requirements to Mexico as of Dec 1, 2016. Thoughts? #isa #immigration\n",
      "@HuffingtonPost people Deep praising Canada and Canada has way stricter immigration laws then us they will boot your liberal American ass\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Supun\\\\Desktop\\\\Supun Backup\\\\Supun\\\\transfer\\\\documents\\\\myfiles\\\\MScDA\\\\DS\\\\IR\\\\Assignment_1\\\\twitter.txt\", \"r\") as f:        # Opening the test file with the intention to read\n",
    "    text = f.read()                     # Reading the file\n",
    "    textBlb = TextBlob(text)            # Making our first textblob\n",
    "    textCorrected = textBlb.correct()   # Correcting the text\n",
    "    print(textCorrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honestly last seven lectures are good. Lectures are understandable. Lecture sides are very useful to self-study also. The given opportunity to ask questions from the lecturer is appreciative. \n",
      "\"Good :) \n",
      "<br />please do reap at class starting it&#039;s better for us.\n",
      "<br />sometimes teaching speed is very high.\n",
      "<br />\n",
      "<br />Thanks ! :)\n",
      "<br /> \"\n",
      "The lectures are good..but a bit speed.A in class working activity is a must one.To please take another hour in thursdays madame.\n",
      "\"\n",
      "<br />He can hear your voice clearly and can understand the things you teach. Presentation sides also good source to refer . of you can do more example questions within the classroom and it will help us to understand the principles well. \n",
      "<br />\"\n",
      "Lectures was well structures and well organized. It was easy to understand. Lecture sides and laws were also well organized. \n",
      "Lectures were good. understandable. \n",
      "The lecture sides were well organized and the examples done in the class helped a lot to learn this new language and also the principles of OOP. Motivate to well. Would have been better if we discussed more about the solutions of coming exercises.   \n",
      "I think i learned a lot from the codes you write in the board. When i compare my codes with yours i can learn about my mistakes and good coming practices that i should follow. There fore i think it would be great if we can discuss more examples in the class.\n",
      "madam explained  the top concepts clearly with examples.lectures were interesting.we want more scenario examples and answers with explanations in future.\n",
      "I satisfy about first 7 lectures. That way of teaching is really good for coming lectures too.\n",
      "lectures are very good. take good effort to make understand every student in the room. very helpful. \n",
      "I was able to obtain a clear picture about OOP and its concepts. \n",
      "\"lecture sides, explanations were very clear.\n",
      "<br />it&#039;s very good to letting ask questions and explain again with suitable examples.\n",
      "<br />sometimes, some codes on white board were unclear at the back.\n",
      "<br />overall very good!!!\n",
      "<br />\"\n",
      "The lectures were good and clear. And they weren&#039;t too fast. Writing code was somewhat confusing because I didn&#039;t know cava before. \n",
      "Actually teaching is very good and can understand easily the concepts by examples which are given in the class.it will be more helpful if provide solved questions as well!. thankyou\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Supun\\\\Desktop\\\\Supun Backup\\\\Supun\\\\transfer\\\\documents\\\\myfiles\\\\MScDA\\\\DS\\\\IR\\\\Assignment_1\\\\student_feedback.txt\", \"r\") as f:        # Opening the test file with the intention to read\n",
    "    text = f.read()                     # Reading the file\n",
    "    textBlb = TextBlob(text)            # Making our first textblob\n",
    "    textCorrected = textBlb.correct()   # Correcting the text\n",
    "    print(textCorrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral network models have shown their promising opportunities for multi-task\n",
      "learning, which focus on learning the shared layers to extract the common and\n",
      "task-variant features. However, in most existing approaches, the extracted shared\n",
      "features are prone to be contaminated by task-specific features or the noise brought\n",
      "by other tasks. In this paper, we propose an adversarial multi-task learning framework,\n",
      "alleviating the shared and private latent feature spaces from interfering with\n",
      "each other. He conduct extensive experiments on 16 different text classification\n",
      "tasks, which demonstrates the benefits of our approach. Besides, we show that the\n",
      "shared knowledge learned by our proposed model can be regarded as off-the-shelf\n",
      "knowledge and easily transferred to new tasks.\n",
      "\n",
      "Multi-task learning is an effective approach to improve the performance of a single task with\n",
      "the help of other related tasks. Recently, neutral-based models for multi-task learning have become\n",
      "very popular, ranging from computer vision (Isa et al., 2016; Hang et al., 2014) to natural\n",
      "language processing (Collobert andWeston, 2008; Long et al., 2015), since they provide a convenient\n",
      "way of combining information from multiple tasks.\n",
      "However, most existing work on multi-task learning (In et al., 2016c,b) attempts to divide the\n",
      "features of different tasks into private and shared spaces, merely based on whether parameter of some components should be shared. Is shown in Figure 1-(a), the general shared-private model introduces\n",
      "two feature spaces for any task: one is used to store task-dependent features, the other is\n",
      "used to capture shared features. The major limitation of this framework is that the shared feature\n",
      "space could contain some unnecessary task-specific features, while some arable features\n",
      "could also be mixed in private space, suffering from feature redundancy.\n",
      "Taking the following two sentences as examples, which are extracted from two different sentiment\n",
      "classification tasks: Movie reviews and Baby products reviews.\n",
      "The infantile cart is simple and easy to use. His kind of humour is infantile and boring.\n",
      "The word ï¿infantile¿½ indicates negative sentiment in Movie task while it is neutral in Baby task.\n",
      "However, the general shared-private model could place the task-specific word ï¿infantile¿½ in a\n",
      "shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space\n",
      "could also be wasted by some unnecessary features.\n",
      "To address this problem, in this paper we propose an adversarial multi-task framework, in\n",
      "which the shared and private feature spaces are in recently dismount by introducing orthogonality constraint.Specifically, we design a genetic shared private learning framework to model the text sequence.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "with open(\"C:\\\\Users\\\\Supun\\\\Desktop\\\\Supun Backup\\\\Supun\\\\transfer\\\\documents\\\\myfiles\\\\MScDA\\\\DS\\\\IR\\\\Assignment_1\\\\research_paper.txt\", \"r\") as f:        # Opening the test file with the intention to read\n",
    "    text = f.read()                     # Reading the file\n",
    "    textBlb = TextBlob(text)            # Making our first textblob\n",
    "    textCorrected = textBlb.correct()   # Correcting the text\n",
    "    print(textCorrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "\n",
    "word_length = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(word_length, prefix_length)\n",
    "print(\"Corpus file not found\") if not sym_spell.create_dictionary(\"C:\\\\Users\\\\Supun\\\\Desktop\\\\Supun Backup\\\\Supun\\\\transfer\\\\documents\\\\myfiles\\\\MScDA\\\\DS\\\\IR\\\\Assignment_1\\\\words.txt\") else print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = 10\n",
    "def correct_tokenized_text(words):\n",
    "    corr_count = 0\n",
    "    corrected_words = []\n",
    "    for i, word in enumerate(words[:-word_length+1]):\n",
    "        word_set = [words[i+j] for j in range(word_length)]\n",
    "        _input = ' '.join(word_set)\n",
    "        result = sym_spell.word_segmentation(_input,2,7)\n",
    "        correction = result.corrected_string\n",
    "        if correction.lower() != _input.lower() and preview < corr_count:\n",
    "            corr_count += 1\n",
    "            print('\"{}\" is corrected as \"{}\"'.format(_input, correction))\n",
    "        corrected_words.append(correction.split(' ')[0])\n",
    "    corrected_words.append(correction.split(' ')[1])\n",
    "    return corrected_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Reminds', 'me', 'of', 'Liberal', 'Immi', 'Frauds', 'Monsey', 'a', 'deport', 'from', 'Canada', 'a', 'cdn', 'lpc', 'cp', 'a', 'a', 'a', 'ttp', 'dimming', 'winter', 'canada', 'ttp', 'We', 'want', 'control', 'immi', 'that', 'contrib', 'positive', 'to', 'the', 'Uk', 'economy', 'a', 'Same', 'as', 'Austral', 'a', 'Canada', 'a', 'ttp', 'Is', 'the', 'new', 'Manitoba', 'immi', 'fee', 'a', 'head', 'tax', 'a', 'ttp', 'Canada', 'immi', 'profit', 'in', 'modern', 'delhi', 'yet', 'abhinaya', 'a', 'Azofy', 'ttp', 'Canada', 'Immi', 'Minister', 'to', 'a', 'a', 'a', 'Subst', 'In', 'Immi', 'Numbers', 'ttp', 'ttp', 'M', 'a', 'a', 'me', 'les', 'dusa', 'up', 'ays', 'dimming', 'par', 'excel', 'Contr', 'a', 'a', 'Le', 'Rigour', 'immi', 'et', 'acc', 'a', 'a', 's', 'a', 'a', 'la', 'green', 'a', 'a', 'a', 'a', 'ttp', 'pshaw', 'what', 'changes', 'should', 'be', 'made', 'to', \"Canada's\", 'immi', 'laws', 'due', 'to', 'the', 'influx', 'of', 'immi', 'and', 'violent', 'a', 'L', 'a', 'a', 'immi', 'irra', 'a', 'a', 'gulix', 'a', 'a', 're', 'au', 'Canada', 'd', 'a', 'a', 'corti', 'a', 'a', 'e', 'en', '5', 'question', 'ttp', 'Immi', 'irra', 'a', 'a', 'gulix', 'a', 'a', 're', 'au', 'Canada', 'd', 'a', 'a', 'corti', 'a', 'a', 'e', 'en', '5', 'question', 'ttp', 'ttp', 'ttp', 'ttp', 'Will', 'Media', 'ask', 'the', 'Liberal', 'if', 'they', 'actual', 'have', 'a', 'solid', 'plan', 'for', 'Canada', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'From', 'my', 'view', 'a', 'immi', 'immi', 'out', 'of', 'C', 'a', 'a', 'a', 'ttp', 'Dan', 'Murray', 'of', 'a', 'a', 'Immi', 'Watch', 'Canada', 'is', 'xeno', 'racist', 'fear', 'liar', 'racism', 'canada', 'cdn', 'hate', 'a', 'a', 'a', 'ttp', 'Le', 'Canada', 'lance', 'une', 'taste', 'camp', 'dimming', 'pour', 'faire', 'face', 'a', 'a', 'son', 'besoin', 'de', 'main', \"d'\", \"'\", 'a', 'a', 'uvre', 'ttp', 'L', 'a', 'a', 'dimming', 'irra', 'a', 'a', 'gulix', 'a', 'a', 're', 'au', 'canada', 'd', 'a', 'a', 'corti', 'a', 'a', 'e', 'en', '5', 'a', 'a', 'question', 'ttp', 'canad', \"I've\", 'read', 'the', 'Immi', 'laws', 'of', 'Canada', 'much', 'strict', 'than', 'the', 'Us', 'Canada', 'Immi', 'Web', 'Traffic', 'Surges', 'And', 'Crashes', 'In', 'Wake', 'Of', 'Trump', 'fast', 'a', 'site', 'a', 'web', 'a', 'a', 'ttp', 'Mr', 'Know', 'of', 'Canada', 'Immi', 'ttp', 'Move', 'to', 'Canada', 'lady', 'Oh', 'a', 'immi', 'rules', 'a', 'you', \"can't\", '...', 'ttp', 'months', 'Annette', 'Toft', 'becomes', \"Canada's\", '2', 'million', 'immi', 'since', '14', 'a', 'Do', 'you', 'know', 'your', \"family's\", 'immi', 'st', 'a', 'a', 'a', 'ttp', 'a', 'thee', 'pro', \"Canada's\", 'open', 'immi', 'police', 'a', 'how', 'they', 'contrib', 'to', 'our', 'economic', 'success', 'a', 'a', 'a', 'a', 'ttp', 'Hundred', 'may', 'lose', 'Canad', 'citizen', 'a', 'resident', 'status', 'because', 'of', 'one', 'corrupt', 'immi', 'consult', 'ttp', 'Immi', 'for', 'canada', 'without', 'india', 'a', 'an', 'compass', 'handle', 'a', 'defy', 'a', 'jamaica', 'immi', 'canada', 'ttp', 'statis', 'dimming', 'a', 'Mexican', 'visa', 'lift', 'expect', 'to', 'cost', 'Canada', 'a', '22', 'over', 'a', 'decade', 'ttp', 'Are', 'people', 'still', 'moving', 'to', 'canada', 'a', 'a', 'a', 'Oh', \"that's\", 'right', 'a', 'they', 'have', 'real', 'immi', 'laws', 'and', \"it's\", 'a', 'a', 'a', 'ttp', 'Here', 'are', 'more', 'details', 'on', 'the', 'Rich', 'a', 'B', 'a', 'C', 'a', 'Immi', 'Consult', 'Sunny', 'Wang', 'who', 'was', 'sentence', 'to', '7', 'years', 'in', '...', 'ttp', 'I', 'added', 'a', 'video', 'to', 'a', 'youth', 'play', 'ttp', 'Funny', 'Talking', 'of', 'Haryana', 'Jat', 'with', 'Canada', 'Immi', 'Girl', 'Agent', 'Mexican', 'Can', 'Now', 'Travel', 'Visa', 'To', 'Canada', 'ttp', 'ttp', 'L', 'a', 'a', 'immi', 'irra', 'a', 'a', 'gulix', 'a', 'a', 're', 'au', 'Canada', 'd', 'a', 'a', 'corti', 'a', 'a', 'e', 'en', '5', 'a', 'a', 'question', 'ttp', 'sweet', 'Hes', 'the', 'Pos', 'that', 'ramped', 'up', 'immi', 'for', 'Canada', 'a', 'among', 'other', 'global', 'police', 'a', 'Canada', 'lifted', 'visa', 'require', 'to', 'Mexico', 'as', 'of', 'Dec', '1', 'a', '20', 'a', 'Thought', 'a', 'visa', 'dimming', 'a', 'people', 'Keep', 'p', 'Canada', 'and', 'Canada', 'has', 'way', 'strict', 'immi', 'laws', 'then', 'us', 'they', 'will', 'boot', 'your', 'liberal', 'America', 'n']\n"
     ]
    }
   ],
   "source": [
    "#twitter data\n",
    "\n",
    "print(correct_tokenized_text(token_twitter))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Honest', 'last', 'seven', 'lecture', 'are', 'good', 'a', 'Lecture', 'are', 'undrest', 'a', 'Lecture', 'slides', 'are', 'very', 'useful', 'to', 'self', 'also', 'a', 'The', 'given', 'apport', 'to', 'ask', 'question', 'from', 'the', 'lecture', 'is', 'apprend', 'a', 'a', 'Good', 'a', 'a', 'a', 'br', 'a', 'a', 'please', 'do', 'recap', 'at', 'class', 'starting', 'it', 'a', 'a', '9', 'a', 's', 'better', 'for', 'us', 'a', 'a', 'br', 'a', 'a', 'some', 'teaching', 'speed', 'is', 'very', 'high', 'a', 'a', 'br', 'a', 'a', 'a', 'br', 'a', 'a', 'Thanks', 'a', 'a', 'a', 'a', 'br', 'a', 'a', 'a', 'The', 'lecture', 'are', 'good', 'a', 'but', 'a', 'bit', 'speed', 'in', 'class', 'working', 'a', 'is', 'a', 'must', \"one's\", 'please', 'take', 'another', 'hour', 'in', 'thurs', \"madame'\", \"''t\", 'a', 'br', 'a', 'a', 'We', 'can', 'hear', 'your', 'voice', 'clearly', 'and', 'can', 'undrest', 'the', 'things', 'you', 'teach', 'a', 'Present', 'slides', 'also', 'good', 'source', 'to', 'refer', 'a', 'lf', 'you', 'can', 'do', 'more', 'example', 'question', 'within', 'the', 'class', 'and', 'it', 'will', 'help', 'us', 'to', 'undrest', 'the', 'principe', 'well', 'a', 'a', 'br', 'a', \"a'\", \"'\", 'Lecture', 'was', 'well', 'struct', 'and', 'well', 'or', 'a', 'It', 'was', 'easy', 'to', 'undrest', 'a', 'Lecture', 'slides', 'and', 'labs', 'were', 'also', 'well', 'or', 'a', 'Lecture', 'were', 'good', 'a', 'undrest', 'a', 'The', 'lecture', 'slides', 'were', 'well', 'or', 'and', 'the', 'example', 'done', 'in', 'the', 'class', 'helped', 'a', 'lot', 'to', 'learn', 'this', 'new', 'language', 'and', 'also', 'the', 'principe', 'of', 'Oop', 'a', 'Motive', 'to', 'well', 'a', 'Would', 'have', 'been', 'better', 'if', 'we', 'discuss', 'more', 'about', 'the', 'solutio', 'of', 'coding', 'exercise', 'a', 'I', 'think', 'i', 'learned', 'a', 'lot', 'from', 'the', 'codes', 'you', 'write', 'in', 'the', 'board', 'a', 'When', 'i', 'compare', 'my', 'codes', 'with', 'yours', 'i', 'can', 'learn', 'about', 'my', 'mistake', 'and', 'good', 'coding', 'practic', 'that', 'i', 'should', 'follow', 'a', 'There', 'fore', 'i', 'think', 'it', 'would', 'be', 'great', 'if', 'we', 'can', 'discuss', 'more', 'example', 'in', 'the', 'class', 'a', 'madam', 'explain', 'the', 'oop', 'concept', 'clearly', 'with', 'example', 'were', 'interest', 'want', 'more', 'scenario', 'example', 'and', 'answers', 'with', 'explain', 'in', 'future', 'a', 'I', 'satisfy', 'about', 'first', '7', 'lecture', 'a', 'That', 'way', 'of', 'teaching', 'is', 'really', 'good', 'for', 'coming', 'lecture', 'too', 'a', 'lecture', 'are', 'very', 'good', 'a', 'take', 'good', 'effort', 'to', 'make', 'under', 'every', 'student', 'in', 'the', 'room', 'a', 'very', 'help', 'a', 'I', 'was', 'able', 'to', 'obtain', 'a', 'clear', 'picture', 'about', 'Oop', 'and', 'its', 'concept', 'a', 'a', 'lecture', 'slides', 'a', 'explain', 'were', 'very', 'clear', 'a', 'a', 'br', 'a', 'a', 'it', 'a', 'a', '9', 'a', 's', 'very', 'good', 'to', 'letting', 'ask', 'question', 'and', 'explain', 'again', 'with', 'suit', 'example', 'a', 'a', 'br', 'a', 'a', 'some', 'a', 'some', 'codes', 'on', 'white', 'board', 'were', 'unclear', 'at', 'the', 'back', 'a', 'a', 'br', 'a', 'a', 'overall', 'very', 'good', 'a', 'a', 'a', 'a', 'br', 'a', \"a'\", \"'\", 'The', 'lecture', 'were', 'good', 'and', 'clear', 'a', 'And', 'they', 'weren', 'a', 'a', '9', 'a', 't', 'too', 'fast', 'a', 'Writing', 'code', 'was', 'some', 'conf', 'because', 'I', 'didn', 'a', 'a', '9', 'a', 't', 'know', 'java', 'before', 'a', 'Actual', 'teaching', 'is', 'very', 'good', 'and', 'can', 'undrest', 'easily', 'the', 'concept', 'by', 'example', 'which', 'are', 'given', 'in', 'the', 'class', 'will', 'be', 'more', 'helpful', 'if', 'provide', 'solved', 'question', 'as', 'well', 'a', 'a', 'thank']\n"
     ]
    }
   ],
   "source": [
    "#student feedback data\n",
    "\n",
    "print(correct_tokenized_text(token_feedback))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neural', 'network', 'models', 'have', 'shown', 'their', 'promise', 'apport', 'for', 'multi', 'learning', 'a', 'which', 'focus', 'on', 'learning', 'the', 'shared', 'layers', 'to', 'extract', 'the', 'common', 'and', 'tasking', 'feature', 'a', 'However', 'a', 'in', 'most', 'existing', 'approach', 'a', 'the', 'extract', 'shared', 'feature', 'are', 'prone', 'to', 'be', 'contam', 'by', 'tasks', 'feature', 'or', 'the', 'noise', 'brought', 'by', 'other', 'tasks', 'a', 'In', 'this', 'paper', 'a', 'we', 'propose', 'an', 'adversa', 'multi', 'learning', 'frame', 'a', 'alluvia', 'the', 'shared', 'and', 'private', 'latent', 'feature', 'spaces', 'from', 'interne', 'with', 'each', 'other', 'a', 'We', 'conduct', 'ex', 'expert', 'on', '16', 'differ', 'text', 'classic', 'tasks', 'a', 'which', 'demon', 'the', 'benefit', 'of', 'our', 'approach', 'a', 'Besides', 'a', 'we', 'show', 'that', 'the', 'shared', 'know', 'learned', 'by', 'our', 'pro', 'model', 'can', 'be', 'regard', 'as', 'off', 'know', 'and', 'easily', 'trans', 'to', 'new', 'tasks', 'a', 'Multi', 'learning', 'is', 'an', 'effect', 'approach', 'to', 'improve', 'the', 'perform', 'of', 'a', 'single', 'task', 'with', 'the', 'help', 'of', 'other', 'related', 'tasks', 'a', 'Recent', 'a', 'neural', 'models', 'for', 'multi', 'learning', 'have', 'become', 'very', 'popular', 'frangi', 'ranging', 'from', 'compute', 'vision', 'a', 'Misura', 'et', 'all', 'a', '20', 'a', 'Zhang', 'et', 'all', 'a', '20', 'a', 'to', 'natural', 'language', 'pro', 'a', 'Coll', 'and', 'a', '1080', 'a', 'Long', 'et', 'all', 'a', '25', 'a', 'a', 'since', 'they', 'provide', 'a', 'convent', 'way', 'of', 'co', 'inform', 'from', 'multiple', 'tasks', 'a', 'However', 'a', 'most', 'existing', 'work', 'on', 'multi', 'learning', 'a', 'Liu', 'et', 'all', 'a', '20', 'a', 'b', 'a', 'at', 'to', 'divide', 'the', 'feature', 'of', 'differ', 'tasks', 'into', 'private', 'and', 'shared', 'spaces', 'a', 'merely', 'based', 'on', 'whether', 'para', 'of', 'some', 'compone', 'should', 'be', 'shared', 'a', 'As', 'shown', 'in', 'Figure', '1', 'a', 'a', 'a', 'a', 'the', 'general', 'shared', 'model', 'intro', 'two', 'feature', 'spaces', 'for', 'any', 'task', 'a', 'one', 'is', 'used', 'to', 'store', 'tasked', 'feature', 'a', 'the', 'other', 'is', 'used', 'to', 'capture', 'shared', 'feature', 'a', 'The', 'major', 'limits', 'of', 'this', 'frame', 'is', 'that', 'the', 'shared', 'feature', 'space', 'could', 'contain', 'some', 'unnice', 'tasks', 'feature', 'a', 'while', 'some', 'sharable', 'feature', 'could', 'also', 'be', 'mixed', 'in', 'private', 'space', 'a', 'suffer', 'from', 'feature', 'red', 'a', 'Taking', 'the', 'follow', 'two', 'sentence', 'as', 'example', 'a', 'which', 'are', 'extract', 'from', 'two', 'differ', 'sentimo', 'classic', 'tasks', 'a', 'Movie', 'reviews', 'and', 'Baby', 'pro', 'reviews', 'a', 'The', 'infant', 'cart', 'is', 'simple', 'and', 'easy', 'to', 'use', 'a', 'This', 'kind', 'of', 'humour', 'is', 'infant', 'and', 'boring', 'a', 'The', 'word', 'infant', 'indicate', 'negative', 'sentimo', 'in', 'Movie', 'task', 'while', 'it', 'is', 'neutral', 'in', 'Baby', 'task', 'a', 'However', 'a', 'the', 'general', 'shared', 'model', 'could', 'place', 'the', 'tasks', 'word', 'infant', 'in', 'a', 'shared', 'space', 'a', 'leaving', 'potent', 'hazards', 'for', 'other', 'tasks', 'a', 'Addition', 'a', 'the', 'capacity', 'of', 'shared', 'space', 'could', 'also', 'be', 'wasted', 'by', 'some', 'unnice', 'feature', 'a', 'To', 'address', 'this', 'problem', 'a', 'in', 'this', 'paper', 'we', 'propose', 'an', 'adversa', 'multi', 'frame', 'a', 'in', 'which', 'the', 'shared', 'and', 'private', 'feature', 'spaces', 'are', 'in', 'hereinto', 'dis', 'by', 'intro', 'ortho', 'constr', 'a', 'we', 'design', 'a', 'generic', 'shared', 'private', 'learning', 'frame', 'to', 'model', 'the', 'text', 's', 'sequence']\n"
     ]
    }
   ],
   "source": [
    "#student feedback data\n",
    "\n",
    "print(correct_tokenized_text(token_research))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reminds: remind\n",
      "me: me\n",
      "of: of\n",
      "Liberal: liber\n",
      "Immigration: immigr\n",
      "Fraudster: fraudster\n",
      "Monsef: monsef\n",
      "avoiding: avoid\n",
      "deportation: deport\n",
      "from: from\n",
      "Canada: canada\n",
      ".: .\n",
      "#cdnpoli: #cdnpoli\n",
      "#LPC: #lpc\n",
      "#CPCLDR: #cpcldr\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/ZOZOSe1CqQ: https://t.co/zozose1cqq\n",
      "#immigration: #immigr\n",
      "#integration: #integr\n",
      "#canada: #canada\n",
      "https://t.co/M5cKGyvV8F: https://t.co/m5ckgyvv8f\n",
      "We: We\n",
      "want: want\n",
      "controlled: control\n",
      "immigration: immigr\n",
      "that: that\n",
      "contributes: contribut\n",
      "positively: posit\n",
      "to: to\n",
      "the: the\n",
      "UK: UK\n",
      "economy: economi\n",
      ".: .\n",
      "Same: same\n",
      "as: as\n",
      "Australia: australia\n",
      "&: &\n",
      "Canada: canada\n",
      ".: .\n",
      "https://t.co/99mYliuOes: https://t.co/99myliuo\n",
      "Is: Is\n",
      "the: the\n",
      "new: new\n",
      "Manitoba: manitoba\n",
      "immigration: immigr\n",
      "fee: fee\n",
      "a: a\n",
      "head: head\n",
      "tax: tax\n",
      "?: ?\n",
      "https://t.co/LsG7C3vLe9: https://t.co/lsg7c3vle9\n",
      "Canada: canada\n",
      "immigration: immigr\n",
      "profit: profit\n",
      "influence: influenc\n",
      "modernistic: modernist\n",
      "delhi: delhi\n",
      "yet: yet\n",
      "abhinav: abhinav\n",
      ":: :\n",
      "XKofy: xkofi\n",
      "https://t.co/becgusY2i6: https://t.co/becgusy2i6\n",
      "Canada: canada\n",
      "Immigration: immigr\n",
      "Minister: minist\n",
      "to: to\n",
      "�: �\n",
      "�: �\n",
      "�: �\n",
      "Substantially: substanti\n",
      "Increase: increas\n",
      "Immigration: immigr\n",
      "Numbers: number\n",
      "https://t.co/nEFw30MRaa: https://t.co/nefw30mraa\n",
      "https://t.co/cyI867PZRV: https://t.co/cyi867pzrv\n",
      "M: M\n",
      "�: �\n",
      "�: �\n",
      "me: me\n",
      "les: le\n",
      "#USA: #usa\n",
      "=p: =p\n",
      "ays: ay\n",
      "d'immigration: d'immigr\n",
      "par: par\n",
      "excellence: excel\n",
      "CONTR: contr\n",
      "�: �\n",
      "�: �\n",
      "LE: LE\n",
      "RIGOUREUSEMENT: rigoureus\n",
      "l'immigration: l'immigr\n",
      "et: et\n",
      "acc: acc\n",
      "�: �\n",
      "�: �\n",
      "s: s\n",
      "�: �\n",
      "�: �\n",
      "la: la\n",
      "#GreenCARD: #greencard\n",
      "!: !\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/IHpVhW2BaG: https://t.co/ihpvhw2bag\n",
      "@Shawhelp: @shawhelp\n",
      "what: what\n",
      "changes: chang\n",
      "should: should\n",
      "be: be\n",
      "made: made\n",
      "to: to\n",
      "Canada's: canada'\n",
      "immigration: immigr\n",
      "laws: law\n",
      "due: due\n",
      "to: to\n",
      "the: the\n",
      "influx: influx\n",
      "of: of\n",
      "immigration: immigr\n",
      "and: and\n",
      "violence: violenc\n",
      "?: ?\n",
      "L: L\n",
      "�: �\n",
      "�: �\n",
      "immigration: immigr\n",
      "irr: irr\n",
      "�: �\n",
      "�: �\n",
      "guli: guli\n",
      "�: �\n",
      "�: �\n",
      "re: re\n",
      "au: au\n",
      "Canada: canada\n",
      "d: d\n",
      "�: �\n",
      "�: �\n",
      "cortiqu: cortiqu\n",
      "�: �\n",
      "�: �\n",
      "e: e\n",
      "en: en\n",
      "5: 5\n",
      "questions: question\n",
      "https://t.co/f4utO5A7ZF: https://t.co/f4uto5a7zf\n",
      "L'immigration: l'immigr\n",
      "irr: irr\n",
      "�: �\n",
      "�: �\n",
      "guli: guli\n",
      "�: �\n",
      "�: �\n",
      "re: re\n",
      "au: au\n",
      "Canada: canada\n",
      "d: d\n",
      "�: �\n",
      "�: �\n",
      "cortiqu: cortiqu\n",
      "�: �\n",
      "�: �\n",
      "e: e\n",
      "en: en\n",
      "5: 5\n",
      "questions: question\n",
      "-: -\n",
      "https://t.co/UiBsEZOqas: https://t.co/uibsezoqa\n",
      "https://t.co/j77dEvjoiX: https://t.co/j77devjoix\n",
      "https://t.co/XXDeIG7Dbu: https://t.co/xxdeig7dbu\n",
      "Will: will\n",
      "Media: media\n",
      "ask: ask\n",
      "the: the\n",
      "Liberals: liber\n",
      "if: if\n",
      "they: they\n",
      "actually: actual\n",
      "have: have\n",
      "a: a\n",
      "solid: solid\n",
      "plan: plan\n",
      "for: for\n",
      "Canada: canada\n",
      "_: _\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "�: �\n",
      "_: _\n",
      "?: ?\n",
      "?: ?\n",
      "From: from\n",
      "my: my\n",
      "view: view\n",
      "-: -\n",
      "-: -\n",
      "immigration: immigr\n",
      "out: out\n",
      "of: of\n",
      "C: C\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/YAgwmZ8ECp: https://t.co/yagwmz8ecp\n",
      "Dan: dan\n",
      "Murray: murray\n",
      "of: of\n",
      "�: �\n",
      "�: �\n",
      "Immigration: immigr\n",
      "Watch: watch\n",
      "Canada: canada\n",
      "is: is\n",
      "xenophobic: xenophob\n",
      "racist: racist\n",
      "fear-mongering: fear-mong\n",
      "liar: liar\n",
      "#racism: #racism\n",
      "#canada: #canada\n",
      "#cdnpoli: #cdnpoli\n",
      "#hatecrime: #hatecrim\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/kwZ3csvYxM: https://t.co/kwz3csvyxm\n",
      "Le: Le\n",
      "Canada: canada\n",
      "lance: lanc\n",
      "une: une\n",
      "vaste: vast\n",
      "campagne: campagn\n",
      "d'immigration: d'immigr\n",
      "pour: pour\n",
      "faire: fair\n",
      "face: face\n",
      "�: �\n",
      "�: �\n",
      "son: son\n",
      "besoin: besoin\n",
      "de: de\n",
      "main: main\n",
      "d: d\n",
      "': '\n",
      "�: �\n",
      "�: �\n",
      "uvre: uvr\n",
      "https://t.co/kXdfMGTZzN: https://t.co/kxdfmgtzzn\n",
      "L: L\n",
      "�: �\n",
      "�: �\n",
      "#immigration: #immigr\n",
      "irr: irr\n",
      "�: �\n",
      "�: �\n",
      "guli: guli\n",
      "�: �\n",
      "�: �\n",
      "re: re\n",
      "au: au\n",
      "#Canada: #canada\n",
      "d: d\n",
      "�: �\n",
      "�: �\n",
      "cortiqu: cortiqu\n",
      "�: �\n",
      "�: �\n",
      "e: e\n",
      "en: en\n",
      "5: 5\n",
      "�: �\n",
      "�: �\n",
      "questions: question\n",
      "https://t.co/s3hu1OKKIG: https://t.co/s3hu1okkig\n",
      "@Canadidly: @canadidli\n",
      "I've: i'v\n",
      "read: read\n",
      "the: the\n",
      "Immigration: immigr\n",
      "laws: law\n",
      "of: of\n",
      "Canada: canada\n",
      "much: much\n",
      "stricter: stricter\n",
      "than: than\n",
      "the: the\n",
      "US: US\n",
      "Canada: canada\n",
      "Immigration: immigr\n",
      "Website: websit\n",
      "Traffic: traffic\n",
      "Surges: surg\n",
      "And: and\n",
      "Crashes: crash\n",
      "In: In\n",
      "Wake: wake\n",
      "Of: Of\n",
      "Trump: trump\n",
      "#fasttraffic: #fasttraff\n",
      ",: ,\n",
      "#sitetraffic: #sitetraff\n",
      ",: ,\n",
      "#website: #websit\n",
      ",: ,\n",
      "#traffic: #traffic\n",
      "https://t.co/zRlJ26jnkC: https://t.co/zrlj26jnkc\n",
      "Mr: Mr\n",
      "Know-all: know-al\n",
      "of: of\n",
      "Canada: canada\n",
      "Immigration: immigr\n",
      "https://t.co/wTQK4QDiKI: https://t.co/wtqk4qdiki\n",
      "Move: move\n",
      "to: to\n",
      "Canada: canada\n",
      "@LadyMadonna___: @ladymadonna___\n",
      "Oh: Oh\n",
      ",: ,\n",
      "immigration: immigr\n",
      "rules: rule\n",
      ",: ,\n",
      "you: you\n",
      "can't: can't\n",
      "...: ...\n",
      "https://t.co/5LIEVHO7A4: https://t.co/5lievho7a4\n",
      "#OnThisDay: #onthisday\n",
      "Annette: annett\n",
      "Toft: toft\n",
      "becomes: becom\n",
      "Canada's: canada'\n",
      "2: 2\n",
      "millionth: millionth\n",
      "immigrant: immigr\n",
      "since: sinc\n",
      "1945: 1945\n",
      ".: .\n",
      "Do: Do\n",
      "you: you\n",
      "know: know\n",
      "your: your\n",
      "family's: family'\n",
      "immigration: immigr\n",
      "st: st\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/UvRuw8eR1b: https://t.co/uvruw8er1b\n",
      ".: .\n",
      "@TheEconomist: @theeconomist\n",
      "profiles: profil\n",
      "Canada's: canada'\n",
      "open: open\n",
      "immigration: immigr\n",
      "policies: polici\n",
      "&: &\n",
      "how: how\n",
      "they: they\n",
      "contribute: contribut\n",
      "to: to\n",
      "our: our\n",
      "economic: econom\n",
      "success: success\n",
      ":: :\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/4K84EE8Y63: https://t.co/4k84ee8y63\n",
      "Hundreds: hundr\n",
      "may: may\n",
      "lose: lose\n",
      "Canadian: canadian\n",
      "citizenship: citizenship\n",
      ",: ,\n",
      "resident: resid\n",
      "status: statu\n",
      "because: becaus\n",
      "of: of\n",
      "one: one\n",
      "corrupt: corrupt\n",
      "immigration: immigr\n",
      "consultant: consult\n",
      "https://t.co/x2IfO0EXI2: https://t.co/x2ifo0exi2\n",
      "Immigration: immigr\n",
      "for: for\n",
      "canada: canada\n",
      "without: without\n",
      "india: india\n",
      ":: :\n",
      "an: an\n",
      "compassionate: compassion\n",
      "handle: handl\n",
      ":: :\n",
      "deyFy: deyfi\n",
      "\": \"\n",
      "#Jamaican: #jamaican\n",
      "#immigrants: #immigr\n",
      "#Canada: #canada\n",
      "https://t.co/vcmfYGadR5: https://t.co/vcmfygadr5\n",
      "#statistics: #statist\n",
      "#immigration: #immigr\n",
      "\": \"\n",
      "Mexican: mexican\n",
      "visa: visa\n",
      "lift: lift\n",
      "expected: expect\n",
      "to: to\n",
      "cost: cost\n",
      "Canada: canada\n",
      "$: $\n",
      "262M: 262m\n",
      "over: over\n",
      "a: a\n",
      "decade: decad\n",
      "https://t.co/9i72fRhtij: https://t.co/9i72frhtij\n",
      "Are: are\n",
      "people: peopl\n",
      "still: still\n",
      "moving: move\n",
      "to: to\n",
      "#Canada: #canada\n",
      "?: ?\n",
      "?: ?\n",
      "?: ?\n",
      "Oh: Oh\n",
      "that's: that'\n",
      "right: right\n",
      ",: ,\n",
      "they: they\n",
      "have: have\n",
      "real: real\n",
      "immigration: immigr\n",
      "laws: law\n",
      "and: and\n",
      "it's: it'\n",
      "�: �\n",
      "�: �\n",
      "_: _\n",
      "https://t.co/0C5OBfmxLG: https://t.co/0c5obfmxlg\n",
      "Here: here\n",
      "are: are\n",
      "more: more\n",
      "details: detail\n",
      "on: on\n",
      "the: the\n",
      "Richmond: richmond\n",
      ",: ,\n",
      "B: B\n",
      ".: .\n",
      "C: C\n",
      ".: .\n",
      "Immigration: immigr\n",
      "Consultant: consult\n",
      "Sunny: sunni\n",
      "Wang: wang\n",
      "who: who\n",
      "was: wa\n",
      "sentenced: sentenc\n",
      "to: to\n",
      "7: 7\n",
      "years: year\n",
      "in: in\n",
      "...: ...\n",
      "https://t.co/YXH5W53srO: https://t.co/yxh5w53sro\n",
      "I: I\n",
      "added: ad\n",
      "a: a\n",
      "video: video\n",
      "to: to\n",
      "a: a\n",
      "@YouTube: @youtub\n",
      "playlist: playlist\n",
      "https://t.co/CnEyWN40x3: https://t.co/cneywn40x3\n",
      "Funny: funni\n",
      "Talking: talk\n",
      "of: of\n",
      "Haryanavi: haryanavi\n",
      "Jat: jat\n",
      "with: with\n",
      "Canada: canada\n",
      "Immigration: immigr\n",
      "Girl: girl\n",
      "Agent: agent\n",
      "Mexicans: mexican\n",
      "Can: can\n",
      "Now: now\n",
      "Travel: travel\n",
      "Visa-Free: visa-fre\n",
      "To: To\n",
      "Canada: canada\n",
      "https://t.co/Ec3XHORO2s: https://t.co/ec3xhoro2\n",
      "https://t.co/RQRr5nebcG: https://t.co/rqrr5nebcg\n",
      "L: L\n",
      "�: �\n",
      "�: �\n",
      "immigration: immigr\n",
      "irr: irr\n",
      "�: �\n",
      "�: �\n",
      "guli: guli\n",
      "�: �\n",
      "�: �\n",
      "re: re\n",
      "au: au\n",
      "Canada: canada\n",
      "d: d\n",
      "�: �\n",
      "�: �\n",
      "cortiqu: cortiqu\n",
      "�: �\n",
      "�: �\n",
      "e: e\n",
      "en: en\n",
      "5: 5\n",
      "�: �\n",
      "�: �\n",
      "questions: question\n",
      "https://t.co/DkpuKyWmaK: https://t.co/dkpukywmak\n",
      "@SweetnessShawnB: @sweetnessshawnb\n",
      "Hes: he\n",
      "the: the\n",
      "POS: po\n",
      "that: that\n",
      "ramped: ramp\n",
      "up: up\n",
      "immigration: immigr\n",
      "for: for\n",
      "Canada: canada\n",
      ",: ,\n",
      "among: among\n",
      "other: other\n",
      "globalist: globalist\n",
      "policies: polici\n",
      ".: .\n",
      "Canada: canada\n",
      "lifted: lift\n",
      "visa: visa\n",
      "requirements: requir\n",
      "to: to\n",
      "Mexico: mexico\n",
      "as: as\n",
      "of: of\n",
      "Dec: dec\n",
      "1: 1\n",
      ",: ,\n",
      "2016: 2016\n",
      ".: .\n",
      "Thoughts: thought\n",
      "?: ?\n",
      "#visa: #visa\n",
      "#immigration: #immigr\n",
      "@HuffingtonPost: @huffingtonpost\n",
      "people: peopl\n",
      "Keep: keep\n",
      "praising: prais\n",
      "Canada: canada\n",
      "and: and\n",
      "Canada: canada\n",
      "has: ha\n",
      "way: way\n",
      "stricter: stricter\n",
      "immigration: immigr\n",
      "laws: law\n",
      "then: then\n",
      "us: us\n",
      "they: they\n",
      "willl: willl\n",
      "boot: boot\n",
      "your: your\n",
      "liberal: liber\n",
      "American: american\n",
      "ass: ass\n"
     ]
    }
   ],
   "source": [
    "twitter_words= token_twitter\n",
    "ps =PorterStemmer()\n",
    "for w in twitter_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(w + \": \" +rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honestly: honestli\n",
      "last: last\n",
      "seven: seven\n",
      "lectures: lectur\n",
      "are: are\n",
      "good: good\n",
      ".: .\n",
      "Lectures: lectur\n",
      "are: are\n",
      "understandable: understand\n",
      ".: .\n",
      "Lecture: lectur\n",
      "slides: slide\n",
      "are: are\n",
      "very: veri\n",
      "useful: use\n",
      "to: to\n",
      "self-study: self-studi\n",
      "also: also\n",
      ".: .\n",
      "The: the\n",
      "given: given\n",
      "opportunity: opportun\n",
      "to: to\n",
      "ask: ask\n",
      "questions: question\n",
      "from: from\n",
      "the: the\n",
      "lecturer: lectur\n",
      "is: is\n",
      "appreciative: appreci\n",
      ".: .\n",
      "``: ``\n",
      "Good: good\n",
      ":: :\n",
      "): )\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "please: pleas\n",
      "do: do\n",
      "recap: recap\n",
      "at: at\n",
      "class: class\n",
      "starting: start\n",
      "it: it\n",
      "&: &\n",
      "#: #\n",
      "039: 039\n",
      ";: ;\n",
      "s: s\n",
      "better: better\n",
      "for: for\n",
      "us: us\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "sometimes: sometim\n",
      "teaching: teach\n",
      "speed: speed\n",
      "is: is\n",
      "very: veri\n",
      "high: high\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "Thanks: thank\n",
      "!: !\n",
      ":: :\n",
      "): )\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "``: ``\n",
      "The: the\n",
      "lectures: lectur\n",
      "are: are\n",
      "good: good\n",
      "..: ..\n",
      "but: but\n",
      "a: a\n",
      "bit: bit\n",
      "speed.A: speed.a\n",
      "in: in\n",
      "class: class\n",
      "working: work\n",
      "activity: activ\n",
      "is: is\n",
      "a: a\n",
      "must: must\n",
      "one.So: one.so\n",
      "please: pleas\n",
      "take: take\n",
      "another: anoth\n",
      "hour: hour\n",
      "in: in\n",
      "thursdays: thursday\n",
      "madame.: madame.\n",
      "'': ''\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "We: We\n",
      "can: can\n",
      "hear: hear\n",
      "your: your\n",
      "voice: voic\n",
      "clearly: clearli\n",
      "and: and\n",
      "can: can\n",
      "understand: understand\n",
      "the: the\n",
      "things: thing\n",
      "you: you\n",
      "teach: teach\n",
      ".: .\n",
      "Presentation: present\n",
      "slides: slide\n",
      "also: also\n",
      "good: good\n",
      "source: sourc\n",
      "to: to\n",
      "refer: refer\n",
      ".: .\n",
      "lf: lf\n",
      "you: you\n",
      "can: can\n",
      "do: do\n",
      "more: more\n",
      "example: exampl\n",
      "questions: question\n",
      "within: within\n",
      "the: the\n",
      "classroom: classroom\n",
      "and: and\n",
      "it: it\n",
      "will: will\n",
      "help: help\n",
      "us: us\n",
      "to: to\n",
      "understand: understand\n",
      "the: the\n",
      "principles: principl\n",
      "well: well\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "'': ''\n",
      "Lectures: lectur\n",
      "was: wa\n",
      "well: well\n",
      "structured: structur\n",
      "and: and\n",
      "well: well\n",
      "organized: organ\n",
      ".: .\n",
      "It: It\n",
      "was: wa\n",
      "easy: easi\n",
      "to: to\n",
      "understand: understand\n",
      ".: .\n",
      "Lecture: lectur\n",
      "slides: slide\n",
      "and: and\n",
      "labs: lab\n",
      "were: were\n",
      "also: also\n",
      "well: well\n",
      "organized: organ\n",
      ".: .\n",
      "Lectures: lectur\n",
      "were: were\n",
      "good: good\n",
      ".: .\n",
      "understandable: understand\n",
      ".: .\n",
      "The: the\n",
      "lecture: lectur\n",
      "slides: slide\n",
      "were: were\n",
      "well: well\n",
      "organized: organ\n",
      "and: and\n",
      "the: the\n",
      "examples: exampl\n",
      "done: done\n",
      "in: in\n",
      "the: the\n",
      "class: class\n",
      "helped: help\n",
      "a: a\n",
      "lot: lot\n",
      "to: to\n",
      "learn: learn\n",
      "this: thi\n",
      "new: new\n",
      "language: languag\n",
      "and: and\n",
      "also: also\n",
      "the: the\n",
      "principles: principl\n",
      "of: of\n",
      "OOP: oop\n",
      ".: .\n",
      "Motivated: motiv\n",
      "to: to\n",
      "well: well\n",
      ".: .\n",
      "Would: would\n",
      "have: have\n",
      "been: been\n",
      "better: better\n",
      "if: if\n",
      "we: we\n",
      "discussed: discuss\n",
      "more: more\n",
      "about: about\n",
      "the: the\n",
      "solutions: solut\n",
      "of: of\n",
      "coding: code\n",
      "exercisers: exercis\n",
      ".: .\n",
      "I: I\n",
      "think: think\n",
      "i: i\n",
      "learned: learn\n",
      "a: a\n",
      "lot: lot\n",
      "from: from\n",
      "the: the\n",
      "codes: code\n",
      "you: you\n",
      "write: write\n",
      "in: in\n",
      "the: the\n",
      "board: board\n",
      ".: .\n",
      "When: when\n",
      "i: i\n",
      "compare: compar\n",
      "my: my\n",
      "codes: code\n",
      "with: with\n",
      "yours: your\n",
      "i: i\n",
      "can: can\n",
      "learn: learn\n",
      "about: about\n",
      "my: my\n",
      "mistakes: mistak\n",
      "and: and\n",
      "good: good\n",
      "coding: code\n",
      "practices: practic\n",
      "that: that\n",
      "i: i\n",
      "should: should\n",
      "follow: follow\n",
      ".: .\n",
      "There: there\n",
      "fore: fore\n",
      "i: i\n",
      "think: think\n",
      "it: it\n",
      "would: would\n",
      "be: be\n",
      "great: great\n",
      "if: if\n",
      "we: we\n",
      "can: can\n",
      "discuss: discuss\n",
      "more: more\n",
      "examples: exampl\n",
      "in: in\n",
      "the: the\n",
      "class: class\n",
      ".: .\n",
      "madam: madam\n",
      "explained: explain\n",
      "the: the\n",
      "oop: oop\n",
      "concepts: concept\n",
      "clearly: clearli\n",
      "with: with\n",
      "examples.lectures: examples.lectur\n",
      "were: were\n",
      "interesting.we: interesting.w\n",
      "want: want\n",
      "more: more\n",
      "scenario: scenario\n",
      "examples: exampl\n",
      "and: and\n",
      "answers: answer\n",
      "with: with\n",
      "explanations: explan\n",
      "in: in\n",
      "future: futur\n",
      ".: .\n",
      "I: I\n",
      "satisfy: satisfi\n",
      "about: about\n",
      "first: first\n",
      "7: 7\n",
      "lectures: lectur\n",
      ".: .\n",
      "That: that\n",
      "way: way\n",
      "of: of\n",
      "teaching: teach\n",
      "is: is\n",
      "really: realli\n",
      "good: good\n",
      "for: for\n",
      "coming: come\n",
      "lectures: lectur\n",
      "too: too\n",
      ".: .\n",
      "lectuers: lectuer\n",
      "are: are\n",
      "very: veri\n",
      "good: good\n",
      ".: .\n",
      "take: take\n",
      "good: good\n",
      "effort: effort\n",
      "to: to\n",
      "make: make\n",
      "undersatand: undersatand\n",
      "every: everi\n",
      "student: student\n",
      "in: in\n",
      "the: the\n",
      "room: room\n",
      ".: .\n",
      "very: veri\n",
      "helpfull: helpful\n",
      ".: .\n",
      "I: I\n",
      "was: wa\n",
      "able: abl\n",
      "to: to\n",
      "obtain: obtain\n",
      "a: a\n",
      "clear: clear\n",
      "picture: pictur\n",
      "about: about\n",
      "OOP: oop\n",
      "and: and\n",
      "its: it\n",
      "concepts: concept\n",
      ".: .\n",
      "``: ``\n",
      "lecture: lectur\n",
      "slides: slide\n",
      ",: ,\n",
      "explanations: explan\n",
      "were: were\n",
      "very: veri\n",
      "clear: clear\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "it: it\n",
      "&: &\n",
      "#: #\n",
      "039: 039\n",
      ";: ;\n",
      "s: s\n",
      "very: veri\n",
      "good: good\n",
      "to: to\n",
      "letting: let\n",
      "ask: ask\n",
      "questions: question\n",
      "and: and\n",
      "explain: explain\n",
      "again: again\n",
      "with: with\n",
      "suitable: suitabl\n",
      "examples: exampl\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "sometimes: sometim\n",
      ",: ,\n",
      "some: some\n",
      "codes: code\n",
      "on: on\n",
      "white: white\n",
      "board: board\n",
      "were: were\n",
      "unclear: unclear\n",
      "at: at\n",
      "the: the\n",
      "back: back\n",
      ".: .\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "overall: overal\n",
      "very: veri\n",
      "good: good\n",
      "!: !\n",
      "!: !\n",
      "!: !\n",
      "<: <\n",
      "br: br\n",
      "/: /\n",
      ">: >\n",
      "'': ''\n",
      "The: the\n",
      "lectures: lectur\n",
      "were: were\n",
      "good: good\n",
      "and: and\n",
      "clear: clear\n",
      ".: .\n",
      "And: and\n",
      "they: they\n",
      "weren: weren\n",
      "&: &\n",
      "#: #\n",
      "039: 039\n",
      ";: ;\n",
      "t: t\n",
      "too: too\n",
      "fast: fast\n",
      ".: .\n",
      "Writing: write\n",
      "code: code\n",
      "was: wa\n",
      "somewhat: somewhat\n",
      "confusing: confus\n",
      "because: becaus\n",
      "I: I\n",
      "didn: didn\n",
      "&: &\n",
      "#: #\n",
      "039: 039\n",
      ";: ;\n",
      "t: t\n",
      "know: know\n",
      "java: java\n",
      "before: befor\n",
      ".: .\n",
      "Actually: actual\n",
      "teaching: teach\n",
      "is: is\n",
      "very: veri\n",
      "good: good\n",
      "and: and\n",
      "can: can\n",
      "understand: understand\n",
      "easily: easili\n",
      "the: the\n",
      "concepts: concept\n",
      "by: by\n",
      "examples: exampl\n",
      "which: which\n",
      "are: are\n",
      "given: given\n",
      "in: in\n",
      "the: the\n",
      "class.it: class.it\n",
      "will: will\n",
      "be: be\n",
      "more: more\n",
      "helpful: help\n",
      "if: if\n",
      "provide: provid\n",
      "solved: solv\n",
      "questions: question\n",
      "as: as\n",
      "well: well\n",
      "!: !\n",
      ".: .\n",
      "thankyou: thankyou\n"
     ]
    }
   ],
   "source": [
    "feedback_words= token_feedback\n",
    "ps =PorterStemmer()\n",
    "for w in feedback_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(w + \": \" +rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural: neural\n",
      "network: network\n",
      "models: model\n",
      "have: have\n",
      "shown: shown\n",
      "their: their\n",
      "promising: promis\n",
      "opportunities: opportun\n",
      "for: for\n",
      "multi-task: multi-task\n",
      "learning: learn\n",
      ",: ,\n",
      "which: which\n",
      "focus: focu\n",
      "on: on\n",
      "learning: learn\n",
      "the: the\n",
      "shared: share\n",
      "layers: layer\n",
      "to: to\n",
      "extract: extract\n",
      "the: the\n",
      "common: common\n",
      "and: and\n",
      "task-invariant: task-invari\n",
      "features: featur\n",
      ".: .\n",
      "However: howev\n",
      ",: ,\n",
      "in: in\n",
      "most: most\n",
      "existing: exist\n",
      "approaches: approach\n",
      ",: ,\n",
      "the: the\n",
      "extracted: extract\n",
      "shared: share\n",
      "features: featur\n",
      "are: are\n",
      "prone: prone\n",
      "to: to\n",
      "be: be\n",
      "contaminated: contamin\n",
      "by: by\n",
      "task-specific: task-specif\n",
      "features: featur\n",
      "or: or\n",
      "the: the\n",
      "noise: nois\n",
      "brought: brought\n",
      "by: by\n",
      "other: other\n",
      "tasks: task\n",
      ".: .\n",
      "In: In\n",
      "this: thi\n",
      "paper: paper\n",
      ",: ,\n",
      "we: we\n",
      "propose: propos\n",
      "an: an\n",
      "adversarial: adversari\n",
      "multi-task: multi-task\n",
      "learning: learn\n",
      "framework: framework\n",
      ",: ,\n",
      "alleviating: allevi\n",
      "the: the\n",
      "shared: share\n",
      "and: and\n",
      "private: privat\n",
      "latent: latent\n",
      "feature: featur\n",
      "spaces: space\n",
      "from: from\n",
      "interfering: interf\n",
      "with: with\n",
      "each: each\n",
      "other: other\n",
      ".: .\n",
      "We: We\n",
      "conduct: conduct\n",
      "extensive: extens\n",
      "experiments: experi\n",
      "on: on\n",
      "16: 16\n",
      "different: differ\n",
      "text: text\n",
      "classification: classif\n",
      "tasks: task\n",
      ",: ,\n",
      "which: which\n",
      "demonstrates: demonstr\n",
      "the: the\n",
      "benefits: benefit\n",
      "of: of\n",
      "our: our\n",
      "approach: approach\n",
      ".: .\n",
      "Besides: besid\n",
      ",: ,\n",
      "we: we\n",
      "show: show\n",
      "that: that\n",
      "the: the\n",
      "shared: share\n",
      "knowledge: knowledg\n",
      "learned: learn\n",
      "by: by\n",
      "our: our\n",
      "proposed: propos\n",
      "model: model\n",
      "can: can\n",
      "be: be\n",
      "regarded: regard\n",
      "as: as\n",
      "off-the-shelf: off-the-shelf\n",
      "knowledge: knowledg\n",
      "and: and\n",
      "easily: easili\n",
      "transferred: transfer\n",
      "to: to\n",
      "new: new\n",
      "tasks: task\n",
      ".: .\n",
      "Multi-task: multi-task\n",
      "learning: learn\n",
      "is: is\n",
      "an: an\n",
      "effective: effect\n",
      "approach: approach\n",
      "to: to\n",
      "improve: improv\n",
      "the: the\n",
      "performance: perform\n",
      "of: of\n",
      "a: a\n",
      "single: singl\n",
      "task: task\n",
      "with: with\n",
      "the: the\n",
      "help: help\n",
      "of: of\n",
      "other: other\n",
      "related: relat\n",
      "tasks: task\n",
      ".: .\n",
      "Recently: recent\n",
      ",: ,\n",
      "neural-based: neural-bas\n",
      "models: model\n",
      "for: for\n",
      "multi-task: multi-task\n",
      "learning: learn\n",
      "have: have\n",
      "become: becom\n",
      "very: veri\n",
      "popular: popular\n",
      ",: ,\n",
      "ranging: rang\n",
      "from: from\n",
      "computer: comput\n",
      "vision: vision\n",
      "(: (\n",
      "Misra: misra\n",
      "et: et\n",
      "al.: al.\n",
      ",: ,\n",
      "2016: 2016\n",
      ";: ;\n",
      "Zhang: zhang\n",
      "et: et\n",
      "al.: al.\n",
      ",: ,\n",
      "2014: 2014\n",
      "): )\n",
      "to: to\n",
      "natural: natur\n",
      "language: languag\n",
      "processing: process\n",
      "(: (\n",
      "Collobert: collobert\n",
      "andWeston: andweston\n",
      ",: ,\n",
      "2008: 2008\n",
      ";: ;\n",
      "Luong: luong\n",
      "et: et\n",
      "al.: al.\n",
      ",: ,\n",
      "2015: 2015\n",
      "): )\n",
      ",: ,\n",
      "since: sinc\n",
      "they: they\n",
      "provide: provid\n",
      "a: a\n",
      "convenient: conveni\n",
      "way: way\n",
      "of: of\n",
      "combining: combin\n",
      "information: inform\n",
      "from: from\n",
      "multiple: multipl\n",
      "tasks: task\n",
      ".: .\n",
      "However: howev\n",
      ",: ,\n",
      "most: most\n",
      "existing: exist\n",
      "work: work\n",
      "on: on\n",
      "multi-task: multi-task\n",
      "learning: learn\n",
      "(: (\n",
      "Liu: liu\n",
      "et: et\n",
      "al.: al.\n",
      ",: ,\n",
      "2016c: 2016c\n",
      ",: ,\n",
      "b: b\n",
      "): )\n",
      "attempts: attempt\n",
      "to: to\n",
      "divide: divid\n",
      "the: the\n",
      "features: featur\n",
      "of: of\n",
      "different: differ\n",
      "tasks: task\n",
      "into: into\n",
      "private: privat\n",
      "and: and\n",
      "shared: share\n",
      "spaces: space\n",
      ",: ,\n",
      "merely: mere\n",
      "based: base\n",
      "on: on\n",
      "whether: whether\n",
      "parameters: paramet\n",
      "of: of\n",
      "some: some\n",
      "components: compon\n",
      "should: should\n",
      "be: be\n",
      "shared: share\n",
      ".: .\n",
      "As: As\n",
      "shown: shown\n",
      "in: in\n",
      "Figure: figur\n",
      "1-: 1-\n",
      "(: (\n",
      "a: a\n",
      "): )\n",
      ",: ,\n",
      "the: the\n",
      "general: gener\n",
      "shared-private: shared-priv\n",
      "model: model\n",
      "introduces: introduc\n",
      "two: two\n",
      "feature: featur\n",
      "spaces: space\n",
      "for: for\n",
      "any: ani\n",
      "task: task\n",
      ":: :\n",
      "one: one\n",
      "is: is\n",
      "used: use\n",
      "to: to\n",
      "store: store\n",
      "task-dependent: task-depend\n",
      "features: featur\n",
      ",: ,\n",
      "the: the\n",
      "other: other\n",
      "is: is\n",
      "used: use\n",
      "to: to\n",
      "capture: captur\n",
      "shared: share\n",
      "features: featur\n",
      ".: .\n",
      "The: the\n",
      "major: major\n",
      "limitation: limit\n",
      "of: of\n",
      "this: thi\n",
      "framework: framework\n",
      "is: is\n",
      "that: that\n",
      "the: the\n",
      "shared: share\n",
      "feature: featur\n",
      "space: space\n",
      "could: could\n",
      "contain: contain\n",
      "some: some\n",
      "unnecessary: unnecessari\n",
      "task-specific: task-specif\n",
      "features: featur\n",
      ",: ,\n",
      "while: while\n",
      "some: some\n",
      "sharable: sharabl\n",
      "features: featur\n",
      "could: could\n",
      "also: also\n",
      "be: be\n",
      "mixed: mix\n",
      "in: in\n",
      "private: privat\n",
      "space: space\n",
      ",: ,\n",
      "suffering: suffer\n",
      "from: from\n",
      "feature: featur\n",
      "redundancy: redund\n",
      ".: .\n",
      "Taking: take\n",
      "the: the\n",
      "following: follow\n",
      "two: two\n",
      "sentences: sentenc\n",
      "as: as\n",
      "examples: exampl\n",
      ",: ,\n",
      "which: which\n",
      "are: are\n",
      "extracted: extract\n",
      "from: from\n",
      "two: two\n",
      "different: differ\n",
      "sentiment: sentiment\n",
      "classification: classif\n",
      "tasks: task\n",
      ":: :\n",
      "Movie: movi\n",
      "reviews: review\n",
      "and: and\n",
      "Baby: babi\n",
      "products: product\n",
      "reviews: review\n",
      ".: .\n",
      "The: the\n",
      "infantile: infantil\n",
      "cart: cart\n",
      "is: is\n",
      "simple: simpl\n",
      "and: and\n",
      "easy: easi\n",
      "to: to\n",
      "use: use\n",
      ".: .\n",
      "This: thi\n",
      "kind: kind\n",
      "of: of\n",
      "humour: humour\n",
      "is: is\n",
      "infantile: infantil\n",
      "and: and\n",
      "boring: bore\n",
      ".: .\n",
      "The: the\n",
      "word: word\n",
      "�infantile�: �infantile�\n",
      "indicates: indic\n",
      "negative: neg\n",
      "sentiment: sentiment\n",
      "in: in\n",
      "Movie: movi\n",
      "task: task\n",
      "while: while\n",
      "it: it\n",
      "is: is\n",
      "neutral: neutral\n",
      "in: in\n",
      "Baby: babi\n",
      "task: task\n",
      ".: .\n",
      "However: howev\n",
      ",: ,\n",
      "the: the\n",
      "general: gener\n",
      "shared-private: shared-priv\n",
      "model: model\n",
      "could: could\n",
      "place: place\n",
      "the: the\n",
      "task-specific: task-specif\n",
      "word: word\n",
      "�infantile�: �infantile�\n",
      "in: in\n",
      "a: a\n",
      "shared: share\n",
      "space: space\n",
      ",: ,\n",
      "leaving: leav\n",
      "potential: potenti\n",
      "hazards: hazard\n",
      "for: for\n",
      "other: other\n",
      "tasks: task\n",
      ".: .\n",
      "Additionally: addit\n",
      ",: ,\n",
      "the: the\n",
      "capacity: capac\n",
      "of: of\n",
      "shared: share\n",
      "space: space\n",
      "could: could\n",
      "also: also\n",
      "be: be\n",
      "wasted: wast\n",
      "by: by\n",
      "some: some\n",
      "unnecessary: unnecessari\n",
      "features: featur\n",
      ".: .\n",
      "To: To\n",
      "address: address\n",
      "this: thi\n",
      "problem: problem\n",
      ",: ,\n",
      "in: in\n",
      "this: thi\n",
      "paper: paper\n",
      "we: we\n",
      "propose: propos\n",
      "an: an\n",
      "adversarial: adversari\n",
      "multi-task: multi-task\n",
      "framework: framework\n",
      ",: ,\n",
      "in: in\n",
      "which: which\n",
      "the: the\n",
      "shared: share\n",
      "and: and\n",
      "private: privat\n",
      "feature: featur\n",
      "spaces: space\n",
      "are: are\n",
      "in: in\n",
      "herently: herent\n",
      "disjoint: disjoint\n",
      "by: by\n",
      "introducing: introduc\n",
      "orthogonality: orthogon\n",
      "constraints.Specifically: constraints.specif\n",
      ",: ,\n",
      "we: we\n",
      "design: design\n",
      "a: a\n",
      "generic: gener\n",
      "shared: share\n",
      "private: privat\n",
      "learning: learn\n",
      "framework: framework\n",
      "to: to\n",
      "model: model\n",
      "the: the\n",
      "text: text\n",
      "sequence: sequenc\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "research_words= token_research\n",
    "ps =PorterStemmer()\n",
    "for w in research_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(w + \": \" +rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Supun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Reminds : Reminds\n",
      "Lemma for me : me\n",
      "Lemma for of : of\n",
      "Lemma for Liberal : Liberal\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Fraudster : Fraudster\n",
      "Lemma for Monsef : Monsef\n",
      "Lemma for avoiding : avoiding\n",
      "Lemma for deportation : deportation\n",
      "Lemma for from : from\n",
      "Lemma for Canada : Canada\n",
      "Lemma for . : .\n",
      "Lemma for # : #\n",
      "Lemma for cdnpoli : cdnpoli\n",
      "Lemma for # : #\n",
      "Lemma for LPC : LPC\n",
      "Lemma for # : #\n",
      "Lemma for CPCLDR��_ : CPCLDR��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/ZOZOSe1CqQ : //t.co/ZOZOSe1CqQ\n",
      "Lemma for # : #\n",
      "Lemma for immigration : immigration\n",
      "Lemma for # : #\n",
      "Lemma for integration : integration\n",
      "Lemma for # : #\n",
      "Lemma for canada : canada\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/M5cKGyvV8F : //t.co/M5cKGyvV8F\n",
      "Lemma for We : We\n",
      "Lemma for want : want\n",
      "Lemma for controlled : controlled\n",
      "Lemma for immigration : immigration\n",
      "Lemma for that : that\n",
      "Lemma for contributes : contributes\n",
      "Lemma for positively : positively\n",
      "Lemma for to : to\n",
      "Lemma for the : the\n",
      "Lemma for UK : UK\n",
      "Lemma for economy : economy\n",
      "Lemma for . : .\n",
      "Lemma for Same : Same\n",
      "Lemma for as : a\n",
      "Lemma for Australia : Australia\n",
      "Lemma for & : &\n",
      "Lemma for amp : amp\n",
      "Lemma for ; : ;\n",
      "Lemma for Canada : Canada\n",
      "Lemma for . : .\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/99mYliuOes : //t.co/99mYliuOes\n",
      "Lemma for Is : Is\n",
      "Lemma for the : the\n",
      "Lemma for new : new\n",
      "Lemma for Manitoba : Manitoba\n",
      "Lemma for immigration : immigration\n",
      "Lemma for fee : fee\n",
      "Lemma for a : a\n",
      "Lemma for head : head\n",
      "Lemma for tax : tax\n",
      "Lemma for ? : ?\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/LsG7C3vLe9 : //t.co/LsG7C3vLe9\n",
      "Lemma for Canada : Canada\n",
      "Lemma for immigration : immigration\n",
      "Lemma for profit : profit\n",
      "Lemma for influence : influence\n",
      "Lemma for modernistic : modernistic\n",
      "Lemma for delhi : delhi\n",
      "Lemma for yet : yet\n",
      "Lemma for abhinav : abhinav\n",
      "Lemma for : : :\n",
      "Lemma for XKofy : XKofy\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/becgusY2i6 : //t.co/becgusY2i6\n",
      "Lemma for Canada : Canada\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Minister : Minister\n",
      "Lemma for to : to\n",
      "Lemma for ���Substantially : ���Substantially\n",
      "Lemma for Increase : Increase\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Numbers : Numbers\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/nEFw30MRaa : //t.co/nEFw30MRaa\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/cyI867PZRV : //t.co/cyI867PZRV\n",
      "Lemma for M��me : M��me\n",
      "Lemma for les : le\n",
      "Lemma for # : #\n",
      "Lemma for USA=pays : USA=pays\n",
      "Lemma for d'immigration : d'immigration\n",
      "Lemma for par : par\n",
      "Lemma for excellence : excellence\n",
      "Lemma for CONTR��LE : CONTR��LE\n",
      "Lemma for RIGOUREUSEMENT : RIGOUREUSEMENT\n",
      "Lemma for l'immigration : l'immigration\n",
      "Lemma for et : et\n",
      "Lemma for acc��s : acc��s\n",
      "Lemma for �� : ��\n",
      "Lemma for la : la\n",
      "Lemma for # : #\n",
      "Lemma for GreenCARD : GreenCARD\n",
      "Lemma for ! : !\n",
      "Lemma for ��_ : ��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/IHpVhW2BaG : //t.co/IHpVhW2BaG\n",
      "Lemma for @ : @\n",
      "Lemma for Shawhelp : Shawhelp\n",
      "Lemma for what : what\n",
      "Lemma for changes : change\n",
      "Lemma for should : should\n",
      "Lemma for be : be\n",
      "Lemma for made : made\n",
      "Lemma for to : to\n",
      "Lemma for Canada : Canada\n",
      "Lemma for 's : 's\n",
      "Lemma for immigration : immigration\n",
      "Lemma for laws : law\n",
      "Lemma for due : due\n",
      "Lemma for to : to\n",
      "Lemma for the : the\n",
      "Lemma for influx : influx\n",
      "Lemma for of : of\n",
      "Lemma for immigration : immigration\n",
      "Lemma for and : and\n",
      "Lemma for violence : violence\n",
      "Lemma for ? : ?\n",
      "Lemma for L��immigration : L��immigration\n",
      "Lemma for irr��guli��re : irr��guli��re\n",
      "Lemma for au : au\n",
      "Lemma for Canada : Canada\n",
      "Lemma for d��cortiqu��e : d��cortiqu��e\n",
      "Lemma for en : en\n",
      "Lemma for 5 : 5\n",
      "Lemma for questions : question\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/f4utO5A7ZF : //t.co/f4utO5A7ZF\n",
      "Lemma for L'immigration : L'immigration\n",
      "Lemma for irr��guli��re : irr��guli��re\n",
      "Lemma for au : au\n",
      "Lemma for Canada : Canada\n",
      "Lemma for d��cortiqu��e : d��cortiqu��e\n",
      "Lemma for en : en\n",
      "Lemma for 5 : 5\n",
      "Lemma for questions : question\n",
      "Lemma for - : -\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/UiBsEZOqas : //t.co/UiBsEZOqas\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/j77dEvjoiX : //t.co/j77dEvjoiX\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/XXDeIG7Dbu : //t.co/XXDeIG7Dbu\n",
      "Lemma for Will : Will\n",
      "Lemma for Media : Media\n",
      "Lemma for ask : ask\n",
      "Lemma for the : the\n",
      "Lemma for Liberals : Liberals\n",
      "Lemma for if : if\n",
      "Lemma for they : they\n",
      "Lemma for actually : actually\n",
      "Lemma for have : have\n",
      "Lemma for a : a\n",
      "Lemma for solid : solid\n",
      "Lemma for plan : plan\n",
      "Lemma for for : for\n",
      "Lemma for Canada : Canada\n",
      "Lemma for _��_�_ : _��_�_\n",
      "Lemma for ? : ?\n",
      "Lemma for ? : ?\n",
      "Lemma for From : From\n",
      "Lemma for my : my\n",
      "Lemma for view : view\n",
      "Lemma for -- : --\n",
      "Lemma for immigration : immigration\n",
      "Lemma for out : out\n",
      "Lemma for of : of\n",
      "Lemma for C��_ : C��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/YAgwmZ8ECp : //t.co/YAgwmZ8ECp\n",
      "Lemma for Dan : Dan\n",
      "Lemma for Murray : Murray\n",
      "Lemma for of��Immigration : of��Immigration\n",
      "Lemma for Watch : Watch\n",
      "Lemma for Canada : Canada\n",
      "Lemma for is : is\n",
      "Lemma for xenophobic : xenophobic\n",
      "Lemma for racist : racist\n",
      "Lemma for fear-mongering : fear-mongering\n",
      "Lemma for liar : liar\n",
      "Lemma for # : #\n",
      "Lemma for racism : racism\n",
      "Lemma for # : #\n",
      "Lemma for canada : canada\n",
      "Lemma for # : #\n",
      "Lemma for cdnpoli : cdnpoli\n",
      "Lemma for # : #\n",
      "Lemma for hatecrime��_ : hatecrime��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/kwZ3csvYxM : //t.co/kwZ3csvYxM\n",
      "Lemma for Le : Le\n",
      "Lemma for Canada : Canada\n",
      "Lemma for lance : lance\n",
      "Lemma for une : une\n",
      "Lemma for vaste : vaste\n",
      "Lemma for campagne : campagne\n",
      "Lemma for d'immigration : d'immigration\n",
      "Lemma for pour : pour\n",
      "Lemma for faire : faire\n",
      "Lemma for face : face\n",
      "Lemma for �� : ��\n",
      "Lemma for son : son\n",
      "Lemma for besoin : besoin\n",
      "Lemma for de : de\n",
      "Lemma for main : main\n",
      "Lemma for d'��uvre : d'��uvre\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/kXdfMGTZzN : //t.co/kXdfMGTZzN\n",
      "Lemma for L�� : L��\n",
      "Lemma for # : #\n",
      "Lemma for immigration : immigration\n",
      "Lemma for irr��guli��re : irr��guli��re\n",
      "Lemma for au : au\n",
      "Lemma for # : #\n",
      "Lemma for Canada : Canada\n",
      "Lemma for d��cortiqu��e : d��cortiqu��e\n",
      "Lemma for en : en\n",
      "Lemma for 5��questions : 5��questions\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/s3hu1OKKIG : //t.co/s3hu1OKKIG\n",
      "Lemma for @ : @\n",
      "Lemma for Canadidly : Canadidly\n",
      "Lemma for I : I\n",
      "Lemma for 've : 've\n",
      "Lemma for read : read\n",
      "Lemma for the : the\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for laws : law\n",
      "Lemma for of : of\n",
      "Lemma for Canada : Canada\n",
      "Lemma for much : much\n",
      "Lemma for stricter : stricter\n",
      "Lemma for than : than\n",
      "Lemma for the : the\n",
      "Lemma for US : US\n",
      "Lemma for Canada : Canada\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Website : Website\n",
      "Lemma for Traffic : Traffic\n",
      "Lemma for Surges : Surges\n",
      "Lemma for And : And\n",
      "Lemma for Crashes : Crashes\n",
      "Lemma for In : In\n",
      "Lemma for Wake : Wake\n",
      "Lemma for Of : Of\n",
      "Lemma for Trump : Trump\n",
      "Lemma for # : #\n",
      "Lemma for fasttraffic : fasttraffic\n",
      "Lemma for , : ,\n",
      "Lemma for # : #\n",
      "Lemma for sitetraffic : sitetraffic\n",
      "Lemma for , : ,\n",
      "Lemma for # : #\n",
      "Lemma for website : website\n",
      "Lemma for , : ,\n",
      "Lemma for # : #\n",
      "Lemma for traffic : traffic\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/zRlJ26jnkC : //t.co/zRlJ26jnkC\n",
      "Lemma for Mr : Mr\n",
      "Lemma for Know-all : Know-all\n",
      "Lemma for of : of\n",
      "Lemma for Canada : Canada\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/wTQK4QDiKI : //t.co/wTQK4QDiKI\n",
      "Lemma for Move : Move\n",
      "Lemma for to : to\n",
      "Lemma for Canada : Canada\n",
      "Lemma for @ : @\n",
      "Lemma for LadyMadonna___ : LadyMadonna___\n",
      "Lemma for Oh : Oh\n",
      "Lemma for , : ,\n",
      "Lemma for immigration : immigration\n",
      "Lemma for rules : rule\n",
      "Lemma for , : ,\n",
      "Lemma for you : you\n",
      "Lemma for ca : ca\n",
      "Lemma for n't : n't\n",
      "Lemma for ... : ...\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/5LIEVHO7A4 : //t.co/5LIEVHO7A4\n",
      "Lemma for # : #\n",
      "Lemma for OnThisDay : OnThisDay\n",
      "Lemma for Annette : Annette\n",
      "Lemma for Toft : Toft\n",
      "Lemma for becomes : becomes\n",
      "Lemma for Canada : Canada\n",
      "Lemma for 's : 's\n",
      "Lemma for 2 : 2\n",
      "Lemma for millionth : millionth\n",
      "Lemma for immigrant : immigrant\n",
      "Lemma for since : since\n",
      "Lemma for 1945 : 1945\n",
      "Lemma for . : .\n",
      "Lemma for Do : Do\n",
      "Lemma for you : you\n",
      "Lemma for know : know\n",
      "Lemma for your : your\n",
      "Lemma for family : family\n",
      "Lemma for 's : 's\n",
      "Lemma for immigration : immigration\n",
      "Lemma for st��_ : st��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/UvRuw8eR1b : //t.co/UvRuw8eR1b\n",
      "Lemma for . : .\n",
      "Lemma for @ : @\n",
      "Lemma for TheEconomist : TheEconomist\n",
      "Lemma for profiles : profile\n",
      "Lemma for Canada : Canada\n",
      "Lemma for 's : 's\n",
      "Lemma for open : open\n",
      "Lemma for immigration : immigration\n",
      "Lemma for policies : policy\n",
      "Lemma for & : &\n",
      "Lemma for amp : amp\n",
      "Lemma for ; : ;\n",
      "Lemma for how : how\n",
      "Lemma for they : they\n",
      "Lemma for contribute : contribute\n",
      "Lemma for to : to\n",
      "Lemma for our : our\n",
      "Lemma for economic : economic\n",
      "Lemma for success : success\n",
      "Lemma for : : :\n",
      "Lemma for ��_ : ��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/4K84EE8Y63 : //t.co/4K84EE8Y63\n",
      "Lemma for Hundreds : Hundreds\n",
      "Lemma for may : may\n",
      "Lemma for lose : lose\n",
      "Lemma for Canadian : Canadian\n",
      "Lemma for citizenship : citizenship\n",
      "Lemma for , : ,\n",
      "Lemma for resident : resident\n",
      "Lemma for status : status\n",
      "Lemma for because : because\n",
      "Lemma for of : of\n",
      "Lemma for one : one\n",
      "Lemma for corrupt : corrupt\n",
      "Lemma for immigration : immigration\n",
      "Lemma for consultant : consultant\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/x2IfO0EXI2 : //t.co/x2IfO0EXI2\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for for : for\n",
      "Lemma for canada : canada\n",
      "Lemma for without : without\n",
      "Lemma for india : india\n",
      "Lemma for : : :\n",
      "Lemma for an : an\n",
      "Lemma for compassionate : compassionate\n",
      "Lemma for handle : handle\n",
      "Lemma for : : :\n",
      "Lemma for deyFy : deyFy\n",
      "Lemma for '' : ''\n",
      "Lemma for # : #\n",
      "Lemma for Jamaican : Jamaican\n",
      "Lemma for # : #\n",
      "Lemma for immigrants : immigrant\n",
      "Lemma for # : #\n",
      "Lemma for Canada : Canada\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/vcmfYGadR5 : //t.co/vcmfYGadR5\n",
      "Lemma for # : #\n",
      "Lemma for statistics : statistic\n",
      "Lemma for # : #\n",
      "Lemma for immigration : immigration\n",
      "Lemma for '' : ''\n",
      "Lemma for Mexican : Mexican\n",
      "Lemma for visa : visa\n",
      "Lemma for lift : lift\n",
      "Lemma for expected : expected\n",
      "Lemma for to : to\n",
      "Lemma for cost : cost\n",
      "Lemma for Canada : Canada\n",
      "Lemma for $ : $\n",
      "Lemma for 262M : 262M\n",
      "Lemma for over : over\n",
      "Lemma for a : a\n",
      "Lemma for decade : decade\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/9i72fRhtij : //t.co/9i72fRhtij\n",
      "Lemma for Are : Are\n",
      "Lemma for people : people\n",
      "Lemma for still : still\n",
      "Lemma for moving : moving\n",
      "Lemma for to : to\n",
      "Lemma for # : #\n",
      "Lemma for Canada : Canada\n",
      "Lemma for ? : ?\n",
      "Lemma for ? : ?\n",
      "Lemma for ? : ?\n",
      "Lemma for Oh : Oh\n",
      "Lemma for that : that\n",
      "Lemma for 's : 's\n",
      "Lemma for right : right\n",
      "Lemma for , : ,\n",
      "Lemma for they : they\n",
      "Lemma for have : have\n",
      "Lemma for real : real\n",
      "Lemma for immigration : immigration\n",
      "Lemma for laws : law\n",
      "Lemma for and : and\n",
      "Lemma for it's��_ : it's��_\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/0C5OBfmxLG : //t.co/0C5OBfmxLG\n",
      "Lemma for Here : Here\n",
      "Lemma for are : are\n",
      "Lemma for more : more\n",
      "Lemma for details : detail\n",
      "Lemma for on : on\n",
      "Lemma for the : the\n",
      "Lemma for Richmond : Richmond\n",
      "Lemma for , : ,\n",
      "Lemma for B.C : B.C\n",
      "Lemma for . : .\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Consultant : Consultant\n",
      "Lemma for Sunny : Sunny\n",
      "Lemma for Wang : Wang\n",
      "Lemma for who : who\n",
      "Lemma for was : wa\n",
      "Lemma for sentenced : sentenced\n",
      "Lemma for to : to\n",
      "Lemma for 7 : 7\n",
      "Lemma for years : year\n",
      "Lemma for in : in\n",
      "Lemma for ... : ...\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/YXH5W53srO : //t.co/YXH5W53srO\n",
      "Lemma for I : I\n",
      "Lemma for added : added\n",
      "Lemma for a : a\n",
      "Lemma for video : video\n",
      "Lemma for to : to\n",
      "Lemma for a : a\n",
      "Lemma for @ : @\n",
      "Lemma for YouTube : YouTube\n",
      "Lemma for playlist : playlist\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/CnEyWN40x3 : //t.co/CnEyWN40x3\n",
      "Lemma for Funny : Funny\n",
      "Lemma for Talking : Talking\n",
      "Lemma for of : of\n",
      "Lemma for Haryanavi : Haryanavi\n",
      "Lemma for Jat : Jat\n",
      "Lemma for with : with\n",
      "Lemma for Canada : Canada\n",
      "Lemma for Immigration : Immigration\n",
      "Lemma for Girl : Girl\n",
      "Lemma for Agent : Agent\n",
      "Lemma for Mexicans : Mexicans\n",
      "Lemma for Can : Can\n",
      "Lemma for Now : Now\n",
      "Lemma for Travel : Travel\n",
      "Lemma for Visa-Free : Visa-Free\n",
      "Lemma for To : To\n",
      "Lemma for Canada : Canada\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/Ec3XHORO2s : //t.co/Ec3XHORO2s\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/RQRr5nebcG : //t.co/RQRr5nebcG\n",
      "Lemma for L��immigration : L��immigration\n",
      "Lemma for irr��guli��re : irr��guli��re\n",
      "Lemma for au : au\n",
      "Lemma for Canada : Canada\n",
      "Lemma for d��cortiqu��e : d��cortiqu��e\n",
      "Lemma for en : en\n",
      "Lemma for 5��questions : 5��questions\n",
      "Lemma for https : http\n",
      "Lemma for : : :\n",
      "Lemma for //t.co/DkpuKyWmaK : //t.co/DkpuKyWmaK\n",
      "Lemma for @ : @\n",
      "Lemma for SweetnessShawnB : SweetnessShawnB\n",
      "Lemma for Hes : Hes\n",
      "Lemma for the : the\n",
      "Lemma for POS : POS\n",
      "Lemma for that : that\n",
      "Lemma for ramped : ramped\n",
      "Lemma for up : up\n",
      "Lemma for immigration : immigration\n",
      "Lemma for for : for\n",
      "Lemma for Canada : Canada\n",
      "Lemma for , : ,\n",
      "Lemma for among : among\n",
      "Lemma for other : other\n",
      "Lemma for globalist : globalist\n",
      "Lemma for policies : policy\n",
      "Lemma for . : .\n",
      "Lemma for Canada : Canada\n",
      "Lemma for lifted : lifted\n",
      "Lemma for visa : visa\n",
      "Lemma for requirements : requirement\n",
      "Lemma for to : to\n",
      "Lemma for Mexico : Mexico\n",
      "Lemma for as : a\n",
      "Lemma for of : of\n",
      "Lemma for Dec : Dec\n",
      "Lemma for 1 : 1\n",
      "Lemma for , : ,\n",
      "Lemma for 2016 : 2016\n",
      "Lemma for . : .\n",
      "Lemma for Thoughts : Thoughts\n",
      "Lemma for ? : ?\n",
      "Lemma for # : #\n",
      "Lemma for visa : visa\n",
      "Lemma for # : #\n",
      "Lemma for immigration : immigration\n",
      "Lemma for @ : @\n",
      "Lemma for HuffingtonPost : HuffingtonPost\n",
      "Lemma for people : people\n",
      "Lemma for Keep : Keep\n",
      "Lemma for praising : praising\n",
      "Lemma for Canada : Canada\n",
      "Lemma for and : and\n",
      "Lemma for Canada : Canada\n",
      "Lemma for has : ha\n",
      "Lemma for way : way\n",
      "Lemma for stricter : stricter\n",
      "Lemma for immigration : immigration\n",
      "Lemma for laws : law\n",
      "Lemma for then : then\n",
      "Lemma for us : u\n",
      "Lemma for they : they\n",
      "Lemma for willl : willl\n",
      "Lemma for boot : boot\n",
      "Lemma for your : your\n",
      "Lemma for liberal : liberal\n",
      "Lemma for American : American\n",
      "Lemma for ass : as\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenization = nltk.word_tokenize(twitterdata)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} : {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Honestly : Honestly\n",
      "Lemma for last : last\n",
      "Lemma for seven : seven\n",
      "Lemma for lectures : lecture\n",
      "Lemma for are : are\n",
      "Lemma for good : good\n",
      "Lemma for . : .\n",
      "Lemma for Lectures : Lectures\n",
      "Lemma for are : are\n",
      "Lemma for understandable : understandable\n",
      "Lemma for . : .\n",
      "Lemma for Lecture : Lecture\n",
      "Lemma for slides : slide\n",
      "Lemma for are : are\n",
      "Lemma for very : very\n",
      "Lemma for useful : useful\n",
      "Lemma for to : to\n",
      "Lemma for self-study : self-study\n",
      "Lemma for also : also\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for given : given\n",
      "Lemma for opportunity : opportunity\n",
      "Lemma for to : to\n",
      "Lemma for ask : ask\n",
      "Lemma for questions : question\n",
      "Lemma for from : from\n",
      "Lemma for the : the\n",
      "Lemma for lecturer : lecturer\n",
      "Lemma for is : is\n",
      "Lemma for appreciative : appreciative\n",
      "Lemma for . : .\n",
      "Lemma for `` : ``\n",
      "Lemma for Good : Good\n",
      "Lemma for : : :\n",
      "Lemma for ) : )\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for please : please\n",
      "Lemma for do : do\n",
      "Lemma for recap : recap\n",
      "Lemma for at : at\n",
      "Lemma for class : class\n",
      "Lemma for starting : starting\n",
      "Lemma for it : it\n",
      "Lemma for & : &\n",
      "Lemma for # : #\n",
      "Lemma for 039 : 039\n",
      "Lemma for ; : ;\n",
      "Lemma for s : s\n",
      "Lemma for better : better\n",
      "Lemma for for : for\n",
      "Lemma for us : u\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for sometimes : sometimes\n",
      "Lemma for teaching : teaching\n",
      "Lemma for speed : speed\n",
      "Lemma for is : is\n",
      "Lemma for very : very\n",
      "Lemma for high : high\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for Thanks : Thanks\n",
      "Lemma for ! : !\n",
      "Lemma for : : :\n",
      "Lemma for ) : )\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for `` : ``\n",
      "Lemma for The : The\n",
      "Lemma for lectures : lecture\n",
      "Lemma for are : are\n",
      "Lemma for good : good\n",
      "Lemma for .. : ..\n",
      "Lemma for but : but\n",
      "Lemma for a : a\n",
      "Lemma for bit : bit\n",
      "Lemma for speed.A : speed.A\n",
      "Lemma for in : in\n",
      "Lemma for class : class\n",
      "Lemma for working : working\n",
      "Lemma for activity : activity\n",
      "Lemma for is : is\n",
      "Lemma for a : a\n",
      "Lemma for must : must\n",
      "Lemma for one.So : one.So\n",
      "Lemma for please : please\n",
      "Lemma for take : take\n",
      "Lemma for another : another\n",
      "Lemma for hour : hour\n",
      "Lemma for in : in\n",
      "Lemma for thursdays : thursday\n",
      "Lemma for madame. : madame.\n",
      "Lemma for '' : ''\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for We : We\n",
      "Lemma for can : can\n",
      "Lemma for hear : hear\n",
      "Lemma for your : your\n",
      "Lemma for voice : voice\n",
      "Lemma for clearly : clearly\n",
      "Lemma for and : and\n",
      "Lemma for can : can\n",
      "Lemma for understand : understand\n",
      "Lemma for the : the\n",
      "Lemma for things : thing\n",
      "Lemma for you : you\n",
      "Lemma for teach : teach\n",
      "Lemma for . : .\n",
      "Lemma for Presentation : Presentation\n",
      "Lemma for slides : slide\n",
      "Lemma for also : also\n",
      "Lemma for good : good\n",
      "Lemma for source : source\n",
      "Lemma for to : to\n",
      "Lemma for refer : refer\n",
      "Lemma for . : .\n",
      "Lemma for lf : lf\n",
      "Lemma for you : you\n",
      "Lemma for can : can\n",
      "Lemma for do : do\n",
      "Lemma for more : more\n",
      "Lemma for example : example\n",
      "Lemma for questions : question\n",
      "Lemma for within : within\n",
      "Lemma for the : the\n",
      "Lemma for classroom : classroom\n",
      "Lemma for and : and\n",
      "Lemma for it : it\n",
      "Lemma for will : will\n",
      "Lemma for help : help\n",
      "Lemma for us : u\n",
      "Lemma for to : to\n",
      "Lemma for understand : understand\n",
      "Lemma for the : the\n",
      "Lemma for principles : principle\n",
      "Lemma for well : well\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for '' : ''\n",
      "Lemma for Lectures : Lectures\n",
      "Lemma for was : wa\n",
      "Lemma for well : well\n",
      "Lemma for structured : structured\n",
      "Lemma for and : and\n",
      "Lemma for well : well\n",
      "Lemma for organized : organized\n",
      "Lemma for . : .\n",
      "Lemma for It : It\n",
      "Lemma for was : wa\n",
      "Lemma for easy : easy\n",
      "Lemma for to : to\n",
      "Lemma for understand : understand\n",
      "Lemma for . : .\n",
      "Lemma for Lecture : Lecture\n",
      "Lemma for slides : slide\n",
      "Lemma for and : and\n",
      "Lemma for labs : lab\n",
      "Lemma for were : were\n",
      "Lemma for also : also\n",
      "Lemma for well : well\n",
      "Lemma for organized : organized\n",
      "Lemma for . : .\n",
      "Lemma for Lectures : Lectures\n",
      "Lemma for were : were\n",
      "Lemma for good : good\n",
      "Lemma for . : .\n",
      "Lemma for understandable : understandable\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for lecture : lecture\n",
      "Lemma for slides : slide\n",
      "Lemma for were : were\n",
      "Lemma for well : well\n",
      "Lemma for organized : organized\n",
      "Lemma for and : and\n",
      "Lemma for the : the\n",
      "Lemma for examples : example\n",
      "Lemma for done : done\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for class : class\n",
      "Lemma for helped : helped\n",
      "Lemma for a : a\n",
      "Lemma for lot : lot\n",
      "Lemma for to : to\n",
      "Lemma for learn : learn\n",
      "Lemma for this : this\n",
      "Lemma for new : new\n",
      "Lemma for language : language\n",
      "Lemma for and : and\n",
      "Lemma for also : also\n",
      "Lemma for the : the\n",
      "Lemma for principles : principle\n",
      "Lemma for of : of\n",
      "Lemma for OOP : OOP\n",
      "Lemma for . : .\n",
      "Lemma for Motivated : Motivated\n",
      "Lemma for to : to\n",
      "Lemma for well : well\n",
      "Lemma for . : .\n",
      "Lemma for Would : Would\n",
      "Lemma for have : have\n",
      "Lemma for been : been\n",
      "Lemma for better : better\n",
      "Lemma for if : if\n",
      "Lemma for we : we\n",
      "Lemma for discussed : discussed\n",
      "Lemma for more : more\n",
      "Lemma for about : about\n",
      "Lemma for the : the\n",
      "Lemma for solutions : solution\n",
      "Lemma for of : of\n",
      "Lemma for coding : coding\n",
      "Lemma for exercisers : exerciser\n",
      "Lemma for . : .\n",
      "Lemma for I : I\n",
      "Lemma for think : think\n",
      "Lemma for i : i\n",
      "Lemma for learned : learned\n",
      "Lemma for a : a\n",
      "Lemma for lot : lot\n",
      "Lemma for from : from\n",
      "Lemma for the : the\n",
      "Lemma for codes : code\n",
      "Lemma for you : you\n",
      "Lemma for write : write\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for board : board\n",
      "Lemma for . : .\n",
      "Lemma for When : When\n",
      "Lemma for i : i\n",
      "Lemma for compare : compare\n",
      "Lemma for my : my\n",
      "Lemma for codes : code\n",
      "Lemma for with : with\n",
      "Lemma for yours : yours\n",
      "Lemma for i : i\n",
      "Lemma for can : can\n",
      "Lemma for learn : learn\n",
      "Lemma for about : about\n",
      "Lemma for my : my\n",
      "Lemma for mistakes : mistake\n",
      "Lemma for and : and\n",
      "Lemma for good : good\n",
      "Lemma for coding : coding\n",
      "Lemma for practices : practice\n",
      "Lemma for that : that\n",
      "Lemma for i : i\n",
      "Lemma for should : should\n",
      "Lemma for follow : follow\n",
      "Lemma for . : .\n",
      "Lemma for There : There\n",
      "Lemma for fore : fore\n",
      "Lemma for i : i\n",
      "Lemma for think : think\n",
      "Lemma for it : it\n",
      "Lemma for would : would\n",
      "Lemma for be : be\n",
      "Lemma for great : great\n",
      "Lemma for if : if\n",
      "Lemma for we : we\n",
      "Lemma for can : can\n",
      "Lemma for discuss : discus\n",
      "Lemma for more : more\n",
      "Lemma for examples : example\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for class : class\n",
      "Lemma for . : .\n",
      "Lemma for madam : madam\n",
      "Lemma for explained : explained\n",
      "Lemma for the : the\n",
      "Lemma for oop : oop\n",
      "Lemma for concepts : concept\n",
      "Lemma for clearly : clearly\n",
      "Lemma for with : with\n",
      "Lemma for examples.lectures : examples.lectures\n",
      "Lemma for were : were\n",
      "Lemma for interesting.we : interesting.we\n",
      "Lemma for want : want\n",
      "Lemma for more : more\n",
      "Lemma for scenario : scenario\n",
      "Lemma for examples : example\n",
      "Lemma for and : and\n",
      "Lemma for answers : answer\n",
      "Lemma for with : with\n",
      "Lemma for explanations : explanation\n",
      "Lemma for in : in\n",
      "Lemma for future : future\n",
      "Lemma for . : .\n",
      "Lemma for I : I\n",
      "Lemma for satisfy : satisfy\n",
      "Lemma for about : about\n",
      "Lemma for first : first\n",
      "Lemma for 7 : 7\n",
      "Lemma for lectures : lecture\n",
      "Lemma for . : .\n",
      "Lemma for That : That\n",
      "Lemma for way : way\n",
      "Lemma for of : of\n",
      "Lemma for teaching : teaching\n",
      "Lemma for is : is\n",
      "Lemma for really : really\n",
      "Lemma for good : good\n",
      "Lemma for for : for\n",
      "Lemma for coming : coming\n",
      "Lemma for lectures : lecture\n",
      "Lemma for too : too\n",
      "Lemma for . : .\n",
      "Lemma for lectuers : lectuers\n",
      "Lemma for are : are\n",
      "Lemma for very : very\n",
      "Lemma for good : good\n",
      "Lemma for . : .\n",
      "Lemma for take : take\n",
      "Lemma for good : good\n",
      "Lemma for effort : effort\n",
      "Lemma for to : to\n",
      "Lemma for make : make\n",
      "Lemma for undersatand : undersatand\n",
      "Lemma for every : every\n",
      "Lemma for student : student\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for room : room\n",
      "Lemma for . : .\n",
      "Lemma for very : very\n",
      "Lemma for helpfull : helpfull\n",
      "Lemma for . : .\n",
      "Lemma for I : I\n",
      "Lemma for was : wa\n",
      "Lemma for able : able\n",
      "Lemma for to : to\n",
      "Lemma for obtain : obtain\n",
      "Lemma for a : a\n",
      "Lemma for clear : clear\n",
      "Lemma for picture : picture\n",
      "Lemma for about : about\n",
      "Lemma for OOP : OOP\n",
      "Lemma for and : and\n",
      "Lemma for its : it\n",
      "Lemma for concepts : concept\n",
      "Lemma for . : .\n",
      "Lemma for `` : ``\n",
      "Lemma for lecture : lecture\n",
      "Lemma for slides : slide\n",
      "Lemma for , : ,\n",
      "Lemma for explanations : explanation\n",
      "Lemma for were : were\n",
      "Lemma for very : very\n",
      "Lemma for clear : clear\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for it : it\n",
      "Lemma for & : &\n",
      "Lemma for # : #\n",
      "Lemma for 039 : 039\n",
      "Lemma for ; : ;\n",
      "Lemma for s : s\n",
      "Lemma for very : very\n",
      "Lemma for good : good\n",
      "Lemma for to : to\n",
      "Lemma for letting : letting\n",
      "Lemma for ask : ask\n",
      "Lemma for questions : question\n",
      "Lemma for and : and\n",
      "Lemma for explain : explain\n",
      "Lemma for again : again\n",
      "Lemma for with : with\n",
      "Lemma for suitable : suitable\n",
      "Lemma for examples : example\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for sometimes : sometimes\n",
      "Lemma for , : ,\n",
      "Lemma for some : some\n",
      "Lemma for codes : code\n",
      "Lemma for on : on\n",
      "Lemma for white : white\n",
      "Lemma for board : board\n",
      "Lemma for were : were\n",
      "Lemma for unclear : unclear\n",
      "Lemma for at : at\n",
      "Lemma for the : the\n",
      "Lemma for back : back\n",
      "Lemma for . : .\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for overall : overall\n",
      "Lemma for very : very\n",
      "Lemma for good : good\n",
      "Lemma for ! : !\n",
      "Lemma for ! : !\n",
      "Lemma for ! : !\n",
      "Lemma for < : <\n",
      "Lemma for br : br\n",
      "Lemma for / : /\n",
      "Lemma for > : >\n",
      "Lemma for '' : ''\n",
      "Lemma for The : The\n",
      "Lemma for lectures : lecture\n",
      "Lemma for were : were\n",
      "Lemma for good : good\n",
      "Lemma for and : and\n",
      "Lemma for clear : clear\n",
      "Lemma for . : .\n",
      "Lemma for And : And\n",
      "Lemma for they : they\n",
      "Lemma for weren : weren\n",
      "Lemma for & : &\n",
      "Lemma for # : #\n",
      "Lemma for 039 : 039\n",
      "Lemma for ; : ;\n",
      "Lemma for t : t\n",
      "Lemma for too : too\n",
      "Lemma for fast : fast\n",
      "Lemma for . : .\n",
      "Lemma for Writing : Writing\n",
      "Lemma for code : code\n",
      "Lemma for was : wa\n",
      "Lemma for somewhat : somewhat\n",
      "Lemma for confusing : confusing\n",
      "Lemma for because : because\n",
      "Lemma for I : I\n",
      "Lemma for didn : didn\n",
      "Lemma for & : &\n",
      "Lemma for # : #\n",
      "Lemma for 039 : 039\n",
      "Lemma for ; : ;\n",
      "Lemma for t : t\n",
      "Lemma for know : know\n",
      "Lemma for java : java\n",
      "Lemma for before : before\n",
      "Lemma for . : .\n",
      "Lemma for Actually : Actually\n",
      "Lemma for teaching : teaching\n",
      "Lemma for is : is\n",
      "Lemma for very : very\n",
      "Lemma for good : good\n",
      "Lemma for and : and\n",
      "Lemma for can : can\n",
      "Lemma for understand : understand\n",
      "Lemma for easily : easily\n",
      "Lemma for the : the\n",
      "Lemma for concepts : concept\n",
      "Lemma for by : by\n",
      "Lemma for examples : example\n",
      "Lemma for which : which\n",
      "Lemma for are : are\n",
      "Lemma for given : given\n",
      "Lemma for in : in\n",
      "Lemma for the : the\n",
      "Lemma for class.it : class.it\n",
      "Lemma for will : will\n",
      "Lemma for be : be\n",
      "Lemma for more : more\n",
      "Lemma for helpful : helpful\n",
      "Lemma for if : if\n",
      "Lemma for provide : provide\n",
      "Lemma for solved : solved\n",
      "Lemma for questions : question\n",
      "Lemma for as : a\n",
      "Lemma for well : well\n",
      "Lemma for ! : !\n",
      "Lemma for . : .\n",
      "Lemma for thankyou : thankyou\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenization = nltk.word_tokenize(feedbackdata)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} : {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for Neural : Neural\n",
      "Lemma for network : network\n",
      "Lemma for models : model\n",
      "Lemma for have : have\n",
      "Lemma for shown : shown\n",
      "Lemma for their : their\n",
      "Lemma for promising : promising\n",
      "Lemma for opportunities : opportunity\n",
      "Lemma for for : for\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for , : ,\n",
      "Lemma for which : which\n",
      "Lemma for focus : focus\n",
      "Lemma for on : on\n",
      "Lemma for learning : learning\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for layers : layer\n",
      "Lemma for to : to\n",
      "Lemma for extract : extract\n",
      "Lemma for the : the\n",
      "Lemma for common : common\n",
      "Lemma for and : and\n",
      "Lemma for task-invariant : task-invariant\n",
      "Lemma for features : feature\n",
      "Lemma for . : .\n",
      "Lemma for However : However\n",
      "Lemma for , : ,\n",
      "Lemma for in : in\n",
      "Lemma for most : most\n",
      "Lemma for existing : existing\n",
      "Lemma for approaches : approach\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for extracted : extracted\n",
      "Lemma for shared : shared\n",
      "Lemma for features : feature\n",
      "Lemma for are : are\n",
      "Lemma for prone : prone\n",
      "Lemma for to : to\n",
      "Lemma for be : be\n",
      "Lemma for contaminated : contaminated\n",
      "Lemma for by : by\n",
      "Lemma for task-specific : task-specific\n",
      "Lemma for features : feature\n",
      "Lemma for or : or\n",
      "Lemma for the : the\n",
      "Lemma for noise : noise\n",
      "Lemma for brought : brought\n",
      "Lemma for by : by\n",
      "Lemma for other : other\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for In : In\n",
      "Lemma for this : this\n",
      "Lemma for paper : paper\n",
      "Lemma for , : ,\n",
      "Lemma for we : we\n",
      "Lemma for propose : propose\n",
      "Lemma for an : an\n",
      "Lemma for adversarial : adversarial\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for framework : framework\n",
      "Lemma for , : ,\n",
      "Lemma for alleviating : alleviating\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for and : and\n",
      "Lemma for private : private\n",
      "Lemma for latent : latent\n",
      "Lemma for feature : feature\n",
      "Lemma for spaces : space\n",
      "Lemma for from : from\n",
      "Lemma for interfering : interfering\n",
      "Lemma for with : with\n",
      "Lemma for each : each\n",
      "Lemma for other : other\n",
      "Lemma for . : .\n",
      "Lemma for We : We\n",
      "Lemma for conduct : conduct\n",
      "Lemma for extensive : extensive\n",
      "Lemma for experiments : experiment\n",
      "Lemma for on : on\n",
      "Lemma for 16 : 16\n",
      "Lemma for different : different\n",
      "Lemma for text : text\n",
      "Lemma for classification : classification\n",
      "Lemma for tasks : task\n",
      "Lemma for , : ,\n",
      "Lemma for which : which\n",
      "Lemma for demonstrates : demonstrates\n",
      "Lemma for the : the\n",
      "Lemma for benefits : benefit\n",
      "Lemma for of : of\n",
      "Lemma for our : our\n",
      "Lemma for approach : approach\n",
      "Lemma for . : .\n",
      "Lemma for Besides : Besides\n",
      "Lemma for , : ,\n",
      "Lemma for we : we\n",
      "Lemma for show : show\n",
      "Lemma for that : that\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for knowledge : knowledge\n",
      "Lemma for learned : learned\n",
      "Lemma for by : by\n",
      "Lemma for our : our\n",
      "Lemma for proposed : proposed\n",
      "Lemma for model : model\n",
      "Lemma for can : can\n",
      "Lemma for be : be\n",
      "Lemma for regarded : regarded\n",
      "Lemma for as : a\n",
      "Lemma for off-the-shelf : off-the-shelf\n",
      "Lemma for knowledge : knowledge\n",
      "Lemma for and : and\n",
      "Lemma for easily : easily\n",
      "Lemma for transferred : transferred\n",
      "Lemma for to : to\n",
      "Lemma for new : new\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for Multi-task : Multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for is : is\n",
      "Lemma for an : an\n",
      "Lemma for effective : effective\n",
      "Lemma for approach : approach\n",
      "Lemma for to : to\n",
      "Lemma for improve : improve\n",
      "Lemma for the : the\n",
      "Lemma for performance : performance\n",
      "Lemma for of : of\n",
      "Lemma for a : a\n",
      "Lemma for single : single\n",
      "Lemma for task : task\n",
      "Lemma for with : with\n",
      "Lemma for the : the\n",
      "Lemma for help : help\n",
      "Lemma for of : of\n",
      "Lemma for other : other\n",
      "Lemma for related : related\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for Recently : Recently\n",
      "Lemma for , : ,\n",
      "Lemma for neural-based : neural-based\n",
      "Lemma for models : model\n",
      "Lemma for for : for\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for have : have\n",
      "Lemma for become : become\n",
      "Lemma for very : very\n",
      "Lemma for popular : popular\n",
      "Lemma for , : ,\n",
      "Lemma for ranging : ranging\n",
      "Lemma for from : from\n",
      "Lemma for computer : computer\n",
      "Lemma for vision : vision\n",
      "Lemma for ( : (\n",
      "Lemma for Misra : Misra\n",
      "Lemma for et : et\n",
      "Lemma for al. : al.\n",
      "Lemma for , : ,\n",
      "Lemma for 2016 : 2016\n",
      "Lemma for ; : ;\n",
      "Lemma for Zhang : Zhang\n",
      "Lemma for et : et\n",
      "Lemma for al. : al.\n",
      "Lemma for , : ,\n",
      "Lemma for 2014 : 2014\n",
      "Lemma for ) : )\n",
      "Lemma for to : to\n",
      "Lemma for natural : natural\n",
      "Lemma for language : language\n",
      "Lemma for processing : processing\n",
      "Lemma for ( : (\n",
      "Lemma for Collobert : Collobert\n",
      "Lemma for andWeston : andWeston\n",
      "Lemma for , : ,\n",
      "Lemma for 2008 : 2008\n",
      "Lemma for ; : ;\n",
      "Lemma for Luong : Luong\n",
      "Lemma for et : et\n",
      "Lemma for al. : al.\n",
      "Lemma for , : ,\n",
      "Lemma for 2015 : 2015\n",
      "Lemma for ) : )\n",
      "Lemma for , : ,\n",
      "Lemma for since : since\n",
      "Lemma for they : they\n",
      "Lemma for provide : provide\n",
      "Lemma for a : a\n",
      "Lemma for convenient : convenient\n",
      "Lemma for way : way\n",
      "Lemma for of : of\n",
      "Lemma for combining : combining\n",
      "Lemma for information : information\n",
      "Lemma for from : from\n",
      "Lemma for multiple : multiple\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for However : However\n",
      "Lemma for , : ,\n",
      "Lemma for most : most\n",
      "Lemma for existing : existing\n",
      "Lemma for work : work\n",
      "Lemma for on : on\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for learning : learning\n",
      "Lemma for ( : (\n",
      "Lemma for Liu : Liu\n",
      "Lemma for et : et\n",
      "Lemma for al. : al.\n",
      "Lemma for , : ,\n",
      "Lemma for 2016c : 2016c\n",
      "Lemma for , : ,\n",
      "Lemma for b : b\n",
      "Lemma for ) : )\n",
      "Lemma for attempts : attempt\n",
      "Lemma for to : to\n",
      "Lemma for divide : divide\n",
      "Lemma for the : the\n",
      "Lemma for features : feature\n",
      "Lemma for of : of\n",
      "Lemma for different : different\n",
      "Lemma for tasks : task\n",
      "Lemma for into : into\n",
      "Lemma for private : private\n",
      "Lemma for and : and\n",
      "Lemma for shared : shared\n",
      "Lemma for spaces : space\n",
      "Lemma for , : ,\n",
      "Lemma for merely : merely\n",
      "Lemma for based : based\n",
      "Lemma for on : on\n",
      "Lemma for whether : whether\n",
      "Lemma for parameters : parameter\n",
      "Lemma for of : of\n",
      "Lemma for some : some\n",
      "Lemma for components : component\n",
      "Lemma for should : should\n",
      "Lemma for be : be\n",
      "Lemma for shared : shared\n",
      "Lemma for . : .\n",
      "Lemma for As : As\n",
      "Lemma for shown : shown\n",
      "Lemma for in : in\n",
      "Lemma for Figure : Figure\n",
      "Lemma for 1- : 1-\n",
      "Lemma for ( : (\n",
      "Lemma for a : a\n",
      "Lemma for ) : )\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for general : general\n",
      "Lemma for shared-private : shared-private\n",
      "Lemma for model : model\n",
      "Lemma for introduces : introduces\n",
      "Lemma for two : two\n",
      "Lemma for feature : feature\n",
      "Lemma for spaces : space\n",
      "Lemma for for : for\n",
      "Lemma for any : any\n",
      "Lemma for task : task\n",
      "Lemma for : : :\n",
      "Lemma for one : one\n",
      "Lemma for is : is\n",
      "Lemma for used : used\n",
      "Lemma for to : to\n",
      "Lemma for store : store\n",
      "Lemma for task-dependent : task-dependent\n",
      "Lemma for features : feature\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for other : other\n",
      "Lemma for is : is\n",
      "Lemma for used : used\n",
      "Lemma for to : to\n",
      "Lemma for capture : capture\n",
      "Lemma for shared : shared\n",
      "Lemma for features : feature\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for major : major\n",
      "Lemma for limitation : limitation\n",
      "Lemma for of : of\n",
      "Lemma for this : this\n",
      "Lemma for framework : framework\n",
      "Lemma for is : is\n",
      "Lemma for that : that\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for feature : feature\n",
      "Lemma for space : space\n",
      "Lemma for could : could\n",
      "Lemma for contain : contain\n",
      "Lemma for some : some\n",
      "Lemma for unnecessary : unnecessary\n",
      "Lemma for task-specific : task-specific\n",
      "Lemma for features : feature\n",
      "Lemma for , : ,\n",
      "Lemma for while : while\n",
      "Lemma for some : some\n",
      "Lemma for sharable : sharable\n",
      "Lemma for features : feature\n",
      "Lemma for could : could\n",
      "Lemma for also : also\n",
      "Lemma for be : be\n",
      "Lemma for mixed : mixed\n",
      "Lemma for in : in\n",
      "Lemma for private : private\n",
      "Lemma for space : space\n",
      "Lemma for , : ,\n",
      "Lemma for suffering : suffering\n",
      "Lemma for from : from\n",
      "Lemma for feature : feature\n",
      "Lemma for redundancy : redundancy\n",
      "Lemma for . : .\n",
      "Lemma for Taking : Taking\n",
      "Lemma for the : the\n",
      "Lemma for following : following\n",
      "Lemma for two : two\n",
      "Lemma for sentences : sentence\n",
      "Lemma for as : a\n",
      "Lemma for examples : example\n",
      "Lemma for , : ,\n",
      "Lemma for which : which\n",
      "Lemma for are : are\n",
      "Lemma for extracted : extracted\n",
      "Lemma for from : from\n",
      "Lemma for two : two\n",
      "Lemma for different : different\n",
      "Lemma for sentiment : sentiment\n",
      "Lemma for classification : classification\n",
      "Lemma for tasks : task\n",
      "Lemma for : : :\n",
      "Lemma for Movie : Movie\n",
      "Lemma for reviews : review\n",
      "Lemma for and : and\n",
      "Lemma for Baby : Baby\n",
      "Lemma for products : product\n",
      "Lemma for reviews : review\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for infantile : infantile\n",
      "Lemma for cart : cart\n",
      "Lemma for is : is\n",
      "Lemma for simple : simple\n",
      "Lemma for and : and\n",
      "Lemma for easy : easy\n",
      "Lemma for to : to\n",
      "Lemma for use : use\n",
      "Lemma for . : .\n",
      "Lemma for This : This\n",
      "Lemma for kind : kind\n",
      "Lemma for of : of\n",
      "Lemma for humour : humour\n",
      "Lemma for is : is\n",
      "Lemma for infantile : infantile\n",
      "Lemma for and : and\n",
      "Lemma for boring : boring\n",
      "Lemma for . : .\n",
      "Lemma for The : The\n",
      "Lemma for word : word\n",
      "Lemma for �infantile� : �infantile�\n",
      "Lemma for indicates : indicates\n",
      "Lemma for negative : negative\n",
      "Lemma for sentiment : sentiment\n",
      "Lemma for in : in\n",
      "Lemma for Movie : Movie\n",
      "Lemma for task : task\n",
      "Lemma for while : while\n",
      "Lemma for it : it\n",
      "Lemma for is : is\n",
      "Lemma for neutral : neutral\n",
      "Lemma for in : in\n",
      "Lemma for Baby : Baby\n",
      "Lemma for task : task\n",
      "Lemma for . : .\n",
      "Lemma for However : However\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for general : general\n",
      "Lemma for shared-private : shared-private\n",
      "Lemma for model : model\n",
      "Lemma for could : could\n",
      "Lemma for place : place\n",
      "Lemma for the : the\n",
      "Lemma for task-specific : task-specific\n",
      "Lemma for word : word\n",
      "Lemma for �infantile� : �infantile�\n",
      "Lemma for in : in\n",
      "Lemma for a : a\n",
      "Lemma for shared : shared\n",
      "Lemma for space : space\n",
      "Lemma for , : ,\n",
      "Lemma for leaving : leaving\n",
      "Lemma for potential : potential\n",
      "Lemma for hazards : hazard\n",
      "Lemma for for : for\n",
      "Lemma for other : other\n",
      "Lemma for tasks : task\n",
      "Lemma for . : .\n",
      "Lemma for Additionally : Additionally\n",
      "Lemma for , : ,\n",
      "Lemma for the : the\n",
      "Lemma for capacity : capacity\n",
      "Lemma for of : of\n",
      "Lemma for shared : shared\n",
      "Lemma for space : space\n",
      "Lemma for could : could\n",
      "Lemma for also : also\n",
      "Lemma for be : be\n",
      "Lemma for wasted : wasted\n",
      "Lemma for by : by\n",
      "Lemma for some : some\n",
      "Lemma for unnecessary : unnecessary\n",
      "Lemma for features : feature\n",
      "Lemma for . : .\n",
      "Lemma for To : To\n",
      "Lemma for address : address\n",
      "Lemma for this : this\n",
      "Lemma for problem : problem\n",
      "Lemma for , : ,\n",
      "Lemma for in : in\n",
      "Lemma for this : this\n",
      "Lemma for paper : paper\n",
      "Lemma for we : we\n",
      "Lemma for propose : propose\n",
      "Lemma for an : an\n",
      "Lemma for adversarial : adversarial\n",
      "Lemma for multi-task : multi-task\n",
      "Lemma for framework : framework\n",
      "Lemma for , : ,\n",
      "Lemma for in : in\n",
      "Lemma for which : which\n",
      "Lemma for the : the\n",
      "Lemma for shared : shared\n",
      "Lemma for and : and\n",
      "Lemma for private : private\n",
      "Lemma for feature : feature\n",
      "Lemma for spaces : space\n",
      "Lemma for are : are\n",
      "Lemma for in : in\n",
      "Lemma for herently : herently\n",
      "Lemma for disjoint : disjoint\n",
      "Lemma for by : by\n",
      "Lemma for introducing : introducing\n",
      "Lemma for orthogonality : orthogonality\n",
      "Lemma for constraints.Specifically : constraints.Specifically\n",
      "Lemma for , : ,\n",
      "Lemma for we : we\n",
      "Lemma for design : design\n",
      "Lemma for a : a\n",
      "Lemma for generic : generic\n",
      "Lemma for shared : shared\n",
      "Lemma for private : private\n",
      "Lemma for learning : learning\n",
      "Lemma for framework : framework\n",
      "Lemma for to : to\n",
      "Lemma for model : model\n",
      "Lemma for the : the\n",
      "Lemma for text : text\n",
      "Lemma for sequence : sequence\n",
      "Lemma for . : .\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokenization = nltk.word_tokenize(researchdata)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} : {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
